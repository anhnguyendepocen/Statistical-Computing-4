out <- tune(svm, oj.train[, !"Purchase"], oj.train$Purchase, ranges = list(cost = cost.grid))
out <- tune(svm, Purchase ~ ., data = oj.train, ranges = list(cost = cost.grid))
summary(out)
svmfit <- out$best.model
table(oj.train$Purchase, predict(svmfit))
mean(oj.train$Purchase == predict(svmfit))
table(oj.test$Purchase, predict(svmfit, newdata = oj.test))
mean(oj.test$Purchase == predict(svmfit, newdata = oj.test))
out <- tune(svm, Purchase ~ ., data = oj.train, kernel = "linear", ranges = list(cost = cost.grid))
summary(out)
svmfit <- out$best.model
table(oj.train$Purchase, predict(svmfit))
mean(oj.train$Purchase == predict(svmfit))
table(oj.test$Purchase, predict(svmfit, newdata = oj.test))
mean(oj.test$Purchase == predict(svmfit, newdata = oj.test))
out <- tune(svm, Purchase ~ ., data = oj.train, kernel = "radix", ranges = list(cost = cost.grid))
out <- tune(svm, Purchase ~ ., data = oj.train, kernel = "radial", ranges = list(cost = cost.grid))
summary(out)
svmfit <- out$best.model
table(oj.train$Purchase, predict(svmfit))
mean(oj.train$Purchase == predict(svmfit))
table(oj.test$Purchase, predict(svmfit, newdata = oj.test))
mean(oj.test$Purchase == predict(svmfit, newdata = oj.test))
# Data Wrangling
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
# Plotting / Graphics
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
# Formatting / Markdown
library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)
# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)
# Resampling & Modeling
library(car, quietly = TRUE, warn.conflicts = FALSE)
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(ISLR, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(boot, quietly = TRUE, warn.conflicts = FALSE)
library(vip, quietly = TRUE, warn.conflicts = FALSE)
options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))
pretty_kable <- function(data, title, dig = 2) {
kable(data, caption = title, digits = dig) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
kableExtra::kable_styling(latex_options = "hold_position")
}
theme_set(theme_light())
# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
legend.title = element_text(size = 12, color = "darkred", face = "bold"),
legend.position = "right", legend.title.align=0.5,
panel.border = element_rect(linetype = "solid",
colour = "lightgray"),
plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))
data.dir <- paste0(here::here(), "/datasets/")
select <- dplyr::select
knitr::opts_chunk$set(comment = NA)
panderOptions('table.alignment.default', function(df)
ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)
load(paste0(here::here(), "ISLR", "10.R.RData")
load(paste0(here::here(), "ISLR", "10.R.RData"))
load(paste0(here::here(), "ISLR/", "10.R.RData"))
load(file(here::here(), "ISLR", "10.R.RData"))
load(file.path(here::here(), "ISLR", "10.R.RData"))
dat <- rbind(x, xtest)
dat <- rbind(x, x.test)
dat
dat.comp <- princomp(dat, scale = T)
?princomp
dat.comp
dat.comp$loadings
dat.comp$loadings[1:5]
dat.comp$loadings
dat.comp$loadings[1]
as.data.table(dat.comp$loadings)
dat.comp$loadings
dat.comp$loadings[1]
dat.comp$loadings[, 1:5]
dat.comp <- princomp(dat, scale = T)
dat.comp <- princomp(dat)
dat.comp
dat.comp <- princomp(dat, scale = TRUE)
dat.comp <- prcomp(dat, scale = TRUE)
dat.comp
dat.comp
dat.comp <- prcomp(dat, scale = TRUE)
dat.comp
dat.comp[1:5]
dat.comp[, 1:5]
dat.comp$x
dat.comp$sdev
dat.comp$x
dat.comp$x[1:5]
dat.comp$x[, 1:5]
sum(dat.comp$x[, 1:5])
dat.comp$x[, 1:5]
dat.comp <- princomp(dat)
dat.comp
dat.comp$loadings
dat.comp$loadings[, 5]
dat.comp$loadings
dat.comp <- prcomp(dat, scale = T)
dat.comp
loadings(dat.comp)
dat.comp
dat.comp$sdev
dat.comp$sdev[1:5]
sum(dat.comp$sdev[1:5])
dat.comp$rotation
dat.comp$center
dat.comp$scale
dat.comp$x
summary(dat.comp)
plot(dat.comp)
par(mfrow = c(1,1))
plot(dat.comp)
pca.out <- prcomp(dat, scale = T)
summary(pca.out) # 0.3499
(pca.out$sdev)^2/ sum(pca.out$sdev^2)
sum( (pca.out$sdev)^2/ sum(pca.out$sdev^2)[1:5] )
(pca.out$sdev)^2/ sum(pca.out$sdev^2)
sum( head((pca.out$sdev)^2/ sum(pca.out$sdev^2)) )
sum(head((pca.out$sdev)^2/ sum(pca.out$sdev^2), length = 5))
screeplot(pca.out)
xols <- pca.out$x[1:300,1:5]
fit0 <- lm(y ~ xols)
summary(fit0)
yhat0 <- predict(fit0, x.test)
mean((yhat0-y.test)^2)
yhat0 <- predict(fit0, x.test)
mean((yhat0-y.test)^2)
states <- row.names(USArrests)
states
names(USArrests)
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
pr.out <- prcomp(USArrests, scale = T)
names(pr.out)
pr.out$center
pr.out$rotation
dim(pr.out$x)
pr.out$rotation =-pr.out$rotation
pr.out$x =-pr.out$x
biplot(pr.out, scale = 0)
pr.out$sdev
pr.var = pr.out$sdev^2
pr.out$sdev
pve <- pr.var/sum(pr.var)
pve
pve
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = "b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
x <- matrix(rnorm(50*2), ncol = 2)
x[1:25, 1] = x[1:25, 1] + 3
x[1:25, 2] = x[1:25, 2] - 4
km.out <- kmeans(x, 2, nstart = 20)
km.out$cluster
plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K=2", pch = 20, cex = 2)
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
hc.complete <- hclust(dist(x), method = "complete")
hc.complete <- hclust(dist(x), method = "complete")
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")
par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage", cex = .9)
plot(hc.average, main = "Average Linkage", cex = .9)
plot(hc.single, main = "Single Linkage", cex = .9)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchial Clustering with Scaled Features")
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchial Clustering with Scaled Features")
par(mfrow = c(1, 1))
x <- matrix(rnorm(30*3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"))
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
set.seed(1)
labels <- sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
labels <- c(1, 1, 1, 2, 2, 2)
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2)
socks <- c(8, 11, 7, 6, 5, 6, 7, 8)
computers <- c(0, 0, 0, 0, 1, 1, 1, 1)
x <- cbind(socks, computers)
labels <- c(1, 1, 2, 2, 2, 2, 1, 1)
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2, asp = 1)
x <- cbind(scale(socks, center = FALSE), scale(computers, center = FALSE))
sd(computers)
labels <- c(1, 1, 2, 2, 2, 2, 1, 1)
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2, asp = 1)
set.seed(1)
Control <- matrix(rnorm(50 * 1000), ncol = 50)
Treatment <- matrix(rnorm(50 * 1000), ncol = 50)
X <- cbind(Control, Treatment)
X[1, ] <- seq(-18, 18 - .36, .36) # linear trend in one dimension
pr.out <- prcomp(scale(X))
summary(pr.out)$importance[, 1]
X <- rbind(X, c(rep(10, 50), rep(0, 50)))
pr.out <- prcomp(scale(X))
summary(pr.out)$importance[, 1]
set.seed(1)
dsc <- scale(USArrests)
d1 <- dist(dsc)^2
d2 <- as.dist(1 - cor(t(dsc)))
summary(d2 / d1)
pr.out <- prcomp(USArrests, scale = TRUE)
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
sum(pr.var)
loadings <- pr.out$rotation
USArrests2 <- scale(USArrests)
sumvar <- sum(apply(as.matrix(USArrests2)^2, 2, sum))
apply((as.matrix(USArrests2) %*% loadings)^2, 2, sum) / sumvar
set.seed(2)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete)
sd.data <- scale(USArrests)
hc.complete.sd <- hclust(dist(sd.data), method = "complete")
plot(hc.complete.sd)
cutree(hc.complete.sd, 3)
table(cutree(hc.complete, 3), cutree(hc.complete.sd, 3))
genes <- read.csv(file.path(here::here(), "ISLR", "Ch10Ex11.csv"), header = FALSE)
install.packages("pdp")
install.packages("iml")
install.packages("DALEX")
install.packages("lime")
# Chunk 1: knitr_setup
knitr::opts_chunk$set(
echo = T,
eval = TRUE,
dev = 'png',
fig.width = 9,
fig.height = 3.5)
options(knitr.table.format = "latex")
# Chunk 2: report_setup
# Data Wrangling
library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)
# Plotting / Graphics
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(visdat, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(pdp, quietly = TRUE, warn.conflicts = FALSE)
library(ROCR, quietly = TRUE, warn.conflicts = FALSE)
library(ggmap, quietly = T, warn.conflicts = FALSE)
library(rpart.plot, quietly = T, warn.conflicts = F)
library(svmpath, quietly = T, warn.conflicts = F)
# Formatting / Markdown
library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)
# Feature Engineering
library(recipes, quietly = TRUE, warn.conflicts = FALSE)
# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)
# Resampling & Modeling
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(h2o, quietly = TRUE, warn.conflicts = FALSE)
library(forecast, quietly = TRUE, warn.conflicts = FALSE)
library(vip, quietly = TRUE, warn.conflicts = FALSE)
library(pdp, quietly = TRUE, warn.conflicts = FALSE)
library(kernlab, quietly = TRUE, warn.conflicts = FALSE)
library(mlbench, quietly = TRUE, warn.conflicts = FALSE)
library(pdp, quietly = TRUE, warn.conflicts = FALSE)
library(iml, quietly = TRUE, warn.conflicts = FALSE)
library(DALEX, quietly = TRUE, warn.conflicts = FALSE)
library(lime, quietly = TRUE, warn.conflicts = FALSE)
# h2o Setup
h2o.no_progress()
h2o.init(strict_version_check = F)
options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))
pretty_kable <- function(data, title, dig = 2) {
kable(data, caption = title, digits = dig) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
kableExtra::kable_styling(latex_options = "hold_position")
}
theme_set(theme_light())
# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
legend.title = element_text(size = 12, color = "darkred", face = "bold"),
legend.position = "right", legend.title.align=0.5,
panel.border = element_rect(linetype = "solid",
colour = "lightgray"),
plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))
data.dir <- paste0(here::here(), "/Hands-On/data/")
select <- dplyr::select # fix clash with MASS
# Set global R options
options(scipen = 999)
# Chunk 3: pander_setup
knitr::opts_chunk$set(comment = NA)
panderOptions('table.alignment.default', function(df)
ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)
# Chunk 4
attrition <- attrition %>% mutate_if(is.ordered, factor, order = F)
attrition_h2o <- as.h2o(attrition)
churn <- initial_split(attrition, prop = .7, strata = "Attrition")
churn_train <- training(churn)
churn_test <- testing(churn)
rm(churn)
# Chunk 5
ames <- AmesHousing::make_ames()
ames_h2o <- as.h2o(ames)
set.seed(123)
ames_split <- initial_split(ames, prop =.7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
rm(ames_split)
h2o.init(max_mem_size = "10g", strict_version_check = F)
train_h2o <- as.h2o(ames_train)
response <- "Sale_Price"
predictors <- setdiff(colnames(ames_train), response)
# Chunk 6
# ensure consistent categorical levels
blueprint <- recipe(Sale_Price ~., data = ames_train) %>%
step_other(all_nominal(), threshold = 0.005)
# Create training / test h2o frames
train_h2o <- prep(blueprint, training = ames_train, retain = T) %>%
juice() %>%
as.h2o()
test_h2o <- prep(blueprint, training = ames_train) %>%
bake(new_data = ames_test) %>%
as.h2o()
Y <- "Sale_Price"
X <- setdiff(names(ames_train), Y)
# Chunk 7
# Train & cross-validate a GLM model
best_glm <- h2o.glm(
x = X, y = Y, training_frame = train_h2o, alpha = 0.1,
remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE, seed = 123
)
# Train & cross-validate a RF model
best_rf <- h2o.randomForest(
x = X, y = Y, training_frame = train_h2o, ntrees = 1000, mtries = 20,
max_depth = 30, min_rows = 1, sample_rate = 0.8, nfolds = 10,
fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
stopping_tolerance = 0
)
# Train & cross-validate a GBM model
best_gbm <- h2o.gbm(
x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.01,
max_depth = 7, min_rows = 5, sample_rate = 0.8, nfolds = 10,
fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,es
seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
stopping_tolerance = 0
)
# Train & cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.05,
max_depth = 3, min_rows = 3, sample_rate = 0.8, categorical_encoding = "Enum",
nfolds = 10, fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE, seed = 123, stopping_rounds = 50,
stopping_metric = "RMSE", stopping_tolerance = 0
)
# Chunk 8
# Train a stacked tree ensemble
ensemble_tree <- h2o.stackedEnsemble(
x = X, y = Y, training_frame = train_h2o, model_id = "my_tree_ensemble",
base_models = list(best_glm, best_rf, best_gbm, best_xgb),
metalearner_algorithm = "drf"
)
# Chunk 9
get_rmse <- function(model) {
results <- h2o.performance(model, newdata = test_h2o)
results@metrics$RMSE
}
list(best_glm, best_rf, best_glm, best_xgb) %>%
purrr::map_dbl(get_rmse)
# Chunk 10
h2o.performance(ensemble_tree, newdata = test_h2o)@metrics$RMSE
# Chunk 11
data.frame(
GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name))
) %>% cor()
# Chunk 12
# Define GBM hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(0.99, 1),
sample_rate = c(0.5, 0.75, 1),
col_sample_rate = c(0.8, 0.9, 1)
)
# Define random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
max_models = 25
)
# Build random grid search
random_grid <- h2o.grid(
algorithm = "gbm", grid_id = "gbm_grid", x = X, y = Y,
training_frame = train_h2o, hyper_params = hyper_grid,
search_criteria = search_criteria, ntrees = 5000, stopping_metric = "RMSE",
stopping_rounds = 10, stopping_tolerance = 0, nfolds = 10,
fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
seed = 123
)
# Chunk 13
# Sort results by RMSE
h2o.getGrid(
grid_id = "gbm_grid",
sort_by = "rmse"
)
# Chunk 14
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
# Chunk 15
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
x = X, y = Y, training_frame = train_h2o, model_id = "ensemble_gbm_grid",
base_models = random_grid@model_ids, metalearner_algorithm = "gbm"
)
# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
# Chunk 16
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
x = X, y = Y, training_frame = train_h2o, nfolds = 5,
max_runtime_secs = 60 * 120, max_models = 50,
keep_cross_validation_predictions = TRUE, sort_metric = "RMSE", seed = 123,
stopping_rounds = 50, stopping_metric = "RMSE", stopping_tolerance = 0
)
# Chunk 17
# Assess the leader board; the following truncates the results to show the top
# 25 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>%
as.data.frame() %>%
dplyr::select(model_id, rmse) %>%
dplyr::slice(1:25)
# Chunk 18
h2o.shutdown(prompt = FALSE)
# Chunk 19
# clean up
rm(list = ls())
library(h2o, quietly = TRUE, warn.conflicts = FALSE)
install.packages("h2o")
