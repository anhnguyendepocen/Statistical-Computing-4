---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Confidence Intervals - The Bootstrap}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 5}
   \fancyfoot[C]{\rmfamily\color{headergrey}Mathematical Statistics}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}
knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")
```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)

assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode, "rtvs"), "data.table")

library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(gtools, quietly = TRUE, warn.conflicts = FALSE)
library(here, quietly = T, warn.conflicts = F)
library(boot, quietly = T, warn.conflicts = F)
library(ggExtra, quietly = T, warn.conflicts = F)
library(GGally, quietly = T, warn.conflicts = F)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/datasets/")
```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

#### 5.1

Consider the samples 1-6. Use a six-sided die to obtain three different bootstrap samples and their corresponding means.

```{r,echo=T}
pop <- seq(from = 1, to = 6, by = 1)

n <- 6

s1 <- mean( sample(pop, n, replace = T) )
s2 <- mean( sample(pop, n, replace = T) )
s3 <- mean( sample(pop, n, replace = T) )

```

$\bar{x}^*_1 = `r s1`$, $\bar{x}^*_2 = `r s2`$, $\bar{x}^*_3 = `r s3`$

#### 5.2

Consider the samples 1, 3, 4, and 6 from some distribution.

```{r, echo = T}

pop <- c(1, 3, 4, 6)

samples <- permutations(n = 4, r = 4, pop, repeats.allowed = T)

```

a.) For one random bootstrap sample, find the probability that the mean is one.

```{r, echo = T}

means <- apply(samples, 1, mean)

p <- mean( means == 1 )
```

Probability: __`r round(p, 4) * 100`%__

b.) For one random bootstrap sample, find the probability that the maximum is 6.

```{r, echo = T}
maxes <- apply(samples, 1, max)

p <- mean( maxes == 6 )
```

Probability: __`r round(p, 4) * 100`%__

c.) For one random bootstrap sample, find the probability that exactly two elements in the sample are less than 2.

```{r, echo = T}

lt2 <- apply(t(apply(samples, 1, function(x) { x < 2})), 1, sum)

p <- mean( lt2 == 2 )
```

Probability: __`r round(p, 4) *100`%__

### 5.3

Consider the sample 1-3.

a.) List all the (ordered) bootstrap samples from this sample. How many are there?

```{r, echo = T}
samples <- permutations(n = 3, r = 3, 1:3, repeats.allowed = T)

n <- nrow(samples)
```

Samples: = $3^3$ = __`r n`__

b.) How many unordered bootstrap samples are there? For example, {1, 2, 2} and {2, 1, 2} are considered to be the same.

```{r}
samples <- combinations(n = 3, r = 3, 1:3, repeats.allowed = T)

n <- nrow(samples)

assertthat::are_equal(n, choose(3 + 3 - 1, 3))
```

Samples: = $5 \choose 3$ = __`r n`__

c.) How many ordered bootstrap samples have one occurrence of 1 and two occurences of 3?

```{r}
samples <- permutations(n = 3, r = 3, 1:3, repeats.allowed = T)

n.ones <- apply(t(apply(samples, 1, FUN = function(x) { x == 1 })), 1, function(x) sum(x) )
n.threes <- apply(t(apply(samples, 1, FUN = function(x) { x == 3 })), 1, function(x) sum(x) )

sum((n.ones == 1 & n.threes == 2) == T)
```

Is this the same number of bootstrap samples that have each of 1, 2 and 3 occuring exactly once?

```{r}

n.ones <- apply(t(apply(samples, 1, FUN = function(x) { x == 1 })), 1, function(x) sum(x) )
n.twos <- apply(t(apply(samples, 1, FUN = function(x) { x == 2 })), 1, function(x) sum(x) )
n.threes <- apply(t(apply(samples, 1, FUN = function(x) { x == 3 })), 1, function(x) sum(x) )

sum((n.ones == 1 & n.twos == 1 & n.threes == 1) == T)
```

No, 3 != 6.

d.) Is the probability of obtaining a bootstrap sample with one 1 and two 3's the same as the probabiliity of obtaining a bootstrap sample with each of 1, 2 and 3 occuring exactly once?

```{r, echo = T}

( sum((n.ones == 1 & n.threes == 2)) / n ) == ( sum((n.ones == 1 & n.twos == 1 &  n.threes == 1)) / n )

```

No, 3% and 6% chances respectfully.

### 5.4

Consider the samples 1, 3, 3, and 5 from some distribution.

```{r}
samples <- c(1, 3, 3, 5)
```

a.) How many bootstrap samples are there?

```{r}
boot <- permutations(n = 3, r = 4, v = samples, repeats.allowed = T)

n <- nrow(boot)
```

_3 unique items to pick from, 4 places to put each item._

Number of permutations: *$3^4$ = `r n`*

b.) List the distinct bootstrap samples assuming order does not matter.

```{r}
combinations(n = 3, r = 4, v = samples, repeats.allowed = T)

choose(4 + 3 - 1, 4)
```

### 5.5

We determine the number of distinct bootstrap samples from a finite set.

a.) A bakery sells five types of cookies: sugar, chocolate chip, oatmeal, peanut butter, and ginger snap. Show that the number of ways to order 5 cookies is $9 \choose 5$

Unordered samping with replacement: $n + k - 1 \choose k$, $n = 5, k = 5$

```{r}
choose(5 + 5 - 1, 5)
```

b.) Show that the number of sets of size n (order does not matter) drawn with replacement from the (distinct) $a_1, a_2, \ldots, a_n$ is ${2n-1}\choose n$

Conclude that the number of distinct bootstrap samples from the set $[a_1, a_2, \ldots, a_n]$ is ${2n - 1} \choose  n$

### 5.6

Let $k_1, k_2, \ldots, k_n$ denote non-negative integers satisfying $k_1 + k_2 + \ldots + k_n = n$, and suppose the elements in the set $a_1, a_2, \ldots, a_n$ are distinct.

a.) Show that the number of bootstrap samples with $k_1$ occurrences of $a_1, k_2$ occurrences of $a_2, \ldots, k_n$ occurrences of $a_n$ is $n \choose k_1, k_2, \ldots, k_n$

b.) Compute the probability that a randomly drawn bootstrap sample will have $k_i$ occurrences of $a_i, i = 1, 2, \ldots, n$

### 5.7

Refer to Example 5.4 and the remark at the end of the example.

a.) What might account for the fact that there were more missing values for the men who skateboarded in front of the male experimenter? How might this bias the outcome?

_It could be that the approached skateboarders refused to participate in the study of performing tricks in front of other men._

b.) Why do you suppose it was important that the two experimenters were blinded to the purpose of the study?

_The female could have flurted or otherwise influenced skateboarders who were performing tricks if they knew the intent of the study._

### 5.8

Consider a population that has a normal distribution with mean $\mu = 36$, standard deviation $\sigma = 8$.

```{r}
mu <- 36; sigma <- 8; n <- 200

se <- mu / sqrt(n)
```

a.) The sampling distribution of $\bar{X}$ for samples of size 200 will have what mean, standard error, and shape?

_$\mu = 36$, SE = 36 / sqrt(200) = `r se`, shape will be approximately normal (CLT)._

b.) Use R to draw a random sample of size 200 from this population. Conduct EDA on your sample.

```{r}
set.seed(123)

samp <- data.table(values = rnorm(200, mean = 36, sd = 8))

ggplot(samp, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), color = "darkorange") +
   geom_vline(xintercept = mu, col = "black", lty = 2) +
   geom_vline(xintercept = mu - se, col = "darkred", lty = 2) +
   geom_vline(xintercept = mu + se, col = "darkred", lty = 2)

```

c.) Compute the bootstrap distribution for your sample, and note the bootstrap mean and standard error.

```{r}

boot.fn <- function(data, index){
   mean(data[index]$values)
}

I <- 10e3

boot(samp, boot.fn, R = I) # boot pkg

cst.boot <- function(values, n, I = 10e3, alpha = 0.05) {
   bootstrap <- numeric(I)
   
   for(i in 1:I)
   {
      bootstrap[i] <- mean( sample(values, n, replace = T) )
   }
   
   observed <- mean(values)
   
   boot.mean <- mean(bootstrap)
   boot.bias <- boot.mean - observed
   boot.se <- sd(bootstrap)
   
   list(bootstrap = bootstrap,
         observed = observed,
         mean = boot.mean,
         bias = boot.bias,
         se = boot.se,
         conf = quantile(bootstrap, c(alpha/2, 1 - alpha/2)))
}

```

d.) Compare the bootstrap distribution to the theoretical sampling distribution by creating a table like Table 5.2:

```{r}
n.200 <- cst.boot(samp$values, 200)

tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, mu, n.200$observed, n.200$mean),
                  SD = c(sigma, mu/sqrt(n), sd(samp$values), n.200$se))

pretty_kable(tbl, "Sampling Statistics")
```

e.) Repeat for sample sizes $n = 50$ and $n = 10$. Carefully describe yyour observations about the effects of sample size on the bootstrap distribuiton.

```{r}
n <- 50
samp <- data.table(values = rnorm(n, mean = mu, sd = sigma))
n.50 <- cst.boot(samp$values, n)

tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, sigma, n.50$observed, n.50$mean),
                  SD = c(sigma, mu/sqrt(n), sd(samp$values), n.50$se))

pretty_kable(tbl, "Sampling Statistics")

```

```{r}
n <- 10
samp <- data.table(values = rnorm(n, mean = mu, sd = sigma))
n.10 <- cst.boot(samp$values, 10)

tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, sigma, n.10$observed, n.10$mean),
                  SD = c(sigma, mu/sqrt(n), sd(samp$values), n.10$se))

pretty_kable(tbl, "Sampling Statistics")
```

_The center of the bootstrap distribution doesn't vary much with smaller n, however, confidence intervals (the sd of the bootstrap dist) vary wildy._

### 5.9

Consider a population that has a gamma distribution with parameters r = 5, $\lambda = 4$.

a.) Use simulation (with $n = 200$) to generate an approximate sampling distribution of the mean; plot and describe the distribution.

```{r}
set.seed(123)

n <- 200; r <- 5; lambda <- 4; mu <- lambda/r

pop <- data.table(values = rgamma(n, shape = lambda, rate = r))

ggplot(pop, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")
```

b.) Now, draw one random sample of size 200 from this population. Create a histogram of your sample, and find the mean and standard deviation.

```{r}
samp <- data.table(values = sample(pop$values, n, replace = T))

ggplot(samp, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")

xbar <- mean(samp$values); sd <- sd(samp$values) 

xbar; sd
```

c.) Compute the bootstrap distribution of the mean for you sample, plot it, and note the bootstrap mean and standard error.

```{r}
I <- 10e3

boot.fn <- function(data, index) {
   mean(data[index]$values)
}

boot(samp, boot.fn, R = I)

n.200 <- cst.boot(samp$values, n)

ggplot(data.table(values = n.200$bootstrap), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange") +
   labs(title = "N=200, bootstrapped mean (Gamma r=5, lambda = 4)")
```

d.) Compare the bootstrap distribution to the approximate theoretical sampling distribution by creating a table like Table 5.2.

```{r}
tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, sd(samp$values), n.200$observed, n.200$mean),
                  SD = c(mu, mu/sqrt(n), sd(samp$values), n.200$se))

pretty_kable(tbl, "Sampling Statistics")
```

e.) Repeat (a-e) for sample sizes of n = 50, and n = 10. Describe carefully your observations about the effects of sample size on the bootstrap distribution.

```{r}
n <- 50

pop <- data.table(values = rgamma(n, shape = lambda, rate = r))

ggplot(pop, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")

samp <- data.table(values = sample(pop$values, n, replace = T))

ggplot(samp, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")

xbar <- mean(samp$values); sd <- sd(samp$values) 

xbar; sd

n.50 <- cst.boot(samp$values, n)

ggplot(data.table(values = n.50$bootstrap), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange") +
   labs(title = "N=50, bootstrapped mean (Gamma r=5, lambda = 4)")

tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, sd(samp$values), n.50$observed, n.50$mean),
                  SD = c(mu, mu/sqrt(n), sd(samp$values), n.50$se))

pretty_kable(tbl, "Sampling Statistics")
```


```{r}
n <- 10

pop <- data.table(values = rgamma(n, shape = lambda, rate = r))

ggplot(pop, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")

samp <- data.table(values = sample(pop$values, n, replace = T))

ggplot(samp, aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")

xbar <- mean(samp$values); sd <- sd(samp$values) 

xbar; sd

n.50 <- cst.boot(samp$values, n)

ggplot(data.table(values = n.50$bootstrap), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange") +
   labs(title = "N=50, bootstrapped mean (Gamma r=5, lambda = 4)")

tbl <- data.table(Data = c("Population", "Sampling Distribution", "Sample", "Bootstrap Distribution"),
                  Mean = c(mu, sd(samp$values), n.50$observed, n.50$mean),
                  SD = c(mu, mu/sqrt(n), sd(samp$values), n.50$se))

pretty_kable(tbl, "Sampling Statistics")
```

### 5.10

We investigate the bootstrap distribution of the median. Create random sample of size n for various n and bootstrap the median. Describe the bootstrap distribution.

```{r}
ne <- 14 # n even
no <- 15 # n odd

wwe <- rnorm(ne) # draw samples of size ne
wwo <- rnorm(no) # draw random samples of size no

N <- 10^4

even.boot <- numeric(N) # save space
odd.boot <- numeric(N)

for(i in 1:N)
{
   x.even <- sample(wwe, ne, replace = T)
   x.odd <- sample(wwo, no, replace = T)
   
   even.boot[i] <- median(x.even)
   odd.boot[i] <- median(x.odd)
}

boot <- data.table(even = even.boot, odd = odd.boot)

p1 <- ggplot(boot, aes(even)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange") +
   labs(title = "even")

p2 <- ggplot(boot, aes(odd)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange") +
   labs(title = "odd")

gridExtra::grid.arrange(p1, p2)
```

Change the sample sizes to 36 and 37; 200 and 201; and 10,000 and 10,001.

Note the similarities/dissimalarities, trends, and so on. Why does the parity of the sample size matter? (_Note: Adjust the x limits in the plots as needed._)

```{r}
ne <- 36 # n even
no <- 37 # n odd

wwe <- rnorm(ne) # draw samples of size ne
wwo <- rnorm(no) # draw random samples of size no

N <- 10^4

even.boot <- numeric(N) # save space
odd.boot <- numeric(N)

for(i in 1:N)
{
   x.even <- sample(wwe, ne, replace = T)
   x.odd <- sample(wwo, no, replace = T)
   
   even.boot[i] <- median(x.even)
   odd.boot[i] <- median(x.odd)
}

boot <- data.table(even = even.boot, odd = odd.boot)

p1 <- ggplot(boot, aes(even)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

p2 <- ggplot(boot, aes(odd)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

gridExtra::grid.arrange(p1, p2)
```

```{r}
ne <- 200 # n even
no <- 201 # n odd

wwe <- rnorm(ne) # draw samples of size ne
wwo <- rnorm(no) # draw random samples of size no

N <- 10^4

even.boot <- numeric(N) # save space
odd.boot <- numeric(N)

for(i in 1:N)
{
   x.even <- sample(wwe, ne, replace = T)
   x.odd <- sample(wwo, no, replace = T)
   
   even.boot[i] <- median(x.even)
   odd.boot[i] <- median(x.odd)
}

boot <- data.table(even = even.boot, odd = odd.boot)

p1 <- ggplot(boot, aes(even)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

p2 <- ggplot(boot, aes(odd)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

gridExtra::grid.arrange(p1, p2)
```

```{r}
ne <- 10000 # n even
no <- 10001 # n odd

wwe <- rnorm(ne) # draw samples of size ne
wwo <- rnorm(no) # draw random samples of size no

N <- 10^4

even.boot <- numeric(N) # save space
odd.boot <- numeric(N)

for(i in 1:N)
{
   x.even <- sample(wwe, ne, replace = T)
   x.odd <- sample(wwo, no, replace = T)
   
   even.boot[i] <- median(x.even)
   odd.boot[i] <- median(x.odd)
}

boot <- data.table(even = even.boot, odd = odd.boot)

p1 <- ggplot(boot, aes(even)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

p2 <- ggplot(boot, aes(odd)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(even), col = "darkorange")

gridExtra::grid.arrange(p1, p2)
```

For odd n, median will be one of the sample points. For smaller n, there will be only n possible values for the median, so the sampling distribution is more "granular" than when n is even.

### 5.11

Import the data from data set Bangladesh. In addition to aresnic concentrations for 271 wells, the data set contains cobolt and chlorine concentrations.

a.) Conduct EDA on the chlorine concentrations and describe the salient features.

```{r}
Bangladesh <- data.table(read.csv(paste0(data.dir, "Bangladesh.csv"),
                                 header = T))
head(Bangladesh)

ggplot(Bangladesh, aes(Chlorine)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(Chlorine), col = "darkorange")

GGally::ggpairs(Bangladesh)
```

b.) Bootstrap the mean.

```{r}
N <- 10e3

boot.fn <- function(data, index){
   mean(data[index], na.rm = T)
}

boot(Bangladesh$Chlorine, boot.fn, R = N)

observed <- mean(Bangladesh$Chlorine, na.rm = T)

bootstrap <- numeric(N)

for(i in 1:N)
{
   bootstrap[i] <- mean(sample(Bangladesh$Chlorine, size = nrow(Bangladesh), replace = T), na.rm = T)   
}

ggplot(data.table(values = bootstrap), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(aes(values), col = "darkorange")
```

c.) Find and interpret the 95% bootstrap percentile confidence interval.

```{r}
alpha <- 0.05
quantile(bootstrap, c(alpha/2, 1 - alpha/2))
```

The spread on the confidence interval is extremely large, which is unsuprising given the heavly skewed distribtion of the sample.

d.) What is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?

```{r}
bias <-  mean(bootstrap) -observed

bias / sd(bootstrap)
```

roughly 1% of the standard error.

### 5.12

Consider Bangladesh chlorine (concentration). Bootstrap the trimmed mean (say, trim the upper and lower 25%), and compare your results with the usual mean (previous result).

```{r}
values <- Bangladesh[!is.na(Chlorine)]$Chlorine

n <- length(values); N <- 10e3; trim <- .25

observed <- mean(values, trim = trim)
bootstrap <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   bootstrap[i] <- mean( sample(values, n, replace = T), trim = trim )
}

ggplot(data.table(value = bootstrap), aes(value)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange")

alpha <- 0.05

boot.mean <- mean(bootstrap)
boot.bias <- boot.mean - observed
boot.se <- sd(bootstrap)

quantile(bootstrap, c(alpha/2, 1 - alpha/2))

# boot pkg

boot.fn <- function(data, index){
   mean( data[index], trim = trim)
}

boot(values, boot.fn, R = N)
```

The bootstrap 20% trimmed mean is substantially smaller than the regular mean. The confidence intervals are also tighter, which is unsuprising due to the heavy presence of outliers in our sample.

### 5.13

The data set _FishMercury_ contains mercury levels (parts per million) for 30 fish caught in lakes in Minnesota.

```{r}
FishMercury <- data.table(read.csv(paste0(data.dir, "FishMercury.csv"),
                                 header = T))
```

a.) Create a histogram or boxplot of the data. What do you observe?

```{r}
ggplot(FishMercury, aes(Mercury)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_rug(col = "darkred")

FishMercury[Mercury > 1.5]

ggplot(FishMercury[Mercury < 1.5], aes(Mercury)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_rug(col = "darkred")

FishMercury[Mercury > 1.5] / sd(FishMercury$Mercury) # 6 sd outlier

```

One extreme (6 SD) outlier in the data.

b.) Bootstrap the mean, and record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
n <- nrow(FishMercury); N <- 10e3

observed <- mean(FishMercury$Mercury)

bootstrap <- vector(mode = "numeric", length = n)

for(i in 1:N)
{
   bootstrap[i] <- mean( sample(FishMercury$Mercury, n, replace = T) )
}

boot.mean <- mean(bootstrap)
boot.bias <- boot.mean - observed
boot.se <- sd(bootstrap)

alpha <- 0.05

quantile(bootstrap, c(alpha/2, 1 - alpha/2))

# boot pkg

boot.fn <- function(data, index) {
   mean( data[index] )
}

boot(FishMercury$Mercury, boot.fn, R = N)
```

c.) Remove the outlier and bootstrap the mean of the remaining data. Record the bootstrap standard error and the 95% bootstrap percentile interval.

```{r}
mercury <- FishMercury[Mercury < 1.5]$Mercury

n <- length(mercury)
bootstrap.values <- vector(mode = "numeric", length = n)

observed <- mean(mercury)

for( i in 1:N)
{
   bootstrap.values[i] <- mean( sample(mercury, size = n, replace = T) )   
}

mean(bootstrap.values)

boot(mercury, boot.fn, R = N)

quantile(bootstrap.values, c(alpha/2, 1 - alpha/2))
```

The standard error reduced drastically, and the confidence intervals are much more narrow.

### 5.14

In Section 3.3, we performed a permutation test to determine if men and women consumed, on average, different amounts of hot wings.

```{r}
BeerWings <- data.table(read.csv(paste0(data.dir, "Beerwings.csv"),
                                 header = T))

wings <- BeerWings$Hotwings

?stat_ecdf

ggplot(BeerWings, aes(Hotwings, group = Gender, col = Gender)) +
   stat_ecdf(geom = "step")

N <- 10e3; alpha <- 0.05; n <- length(wings)

wings.m <- BeerWings[Gender == "M"]$Hotwings
wings.f <- BeerWings[Gender == "F"]$Hotwings

observed <- mean(wings.m) - mean(wings.f)

permutation.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp <- sample(1:n, length(wings.m), replace = F)
   
   samp.m <- wings[samp]; samp.w <- wings[-samp]
   
   permutation.values[i] <- mean(samp.m) - mean(samp.w)
}

ggplot(data.table(values = permutation.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

ggplot(data.table(value = permutation.values), aes(value)) +
   stat_ecdf(geom = "step") +
   geom_vline(xintercept = observed, col = "darkred") +
   scale_y_continuous(labels = comma)

p <- (sum(permutation.values >= observed) + 1) / (N + 1)

var <- p*(1 - p)/N
```

a.) Bootstrap the difference in means and describe the bootstrap distribution.

```{r}
bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp.m <- sample(wings.m, size = length(wings.m), replace = T)
   samp.f <- sample(wings.f, size = length(wings.f), replace = T)
   
   bootstrap.values[i] <- mean(samp.m) - mean(samp.f)
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30)

boot.mean <- mean(bootstrap.values)
boot.se <- sd(bootstrap.values)

boot.bias <- boot.mean - observed

boot.se; boot.bias
```

b.) Find a 95% bootstrap percentile confidence interval for the difference of means, and give a sentence interpreting this interval.

```{r}
quantile(bootstrap.values, c(alpha/2, 1 - alpha/2))
```

The 95% confidence interval does not contain 0, further supporting our permutation test results.

c.) How do the bootstrap and permutation distributions differ?

The permutation distribution is sampled without replacement, and the bootstrap distribution is sampled using replacement.

### 5.15

A high school student was curious about the total number of minutes devoted to commercials during any given half-hour time period on basic and extended cable TV channels. (B. Rodgers and T. Robinson, personal communication).

Import the TV data.

```{r}
TV <- data.table(read.csv(paste0(data.dir, "TV.csv"),
                                 header = T))
```

a.) Perform some exploratory data analysis and obtain summary statistics on the commercial times on basic and extended cable TV channels (do seperate analysis for each type of channel).

```{r}
ggplot(TV, aes(Cable, Times, fill = Cable)) +
   geom_boxplot()

ggplot(TV, aes(Times, group = Cable)) +
   geom_density(aes(y = ..density.., fill = Cable), alpha = .45)

aov(TV$Times ~ TV$Cable)

tv.basic <- TV[Cable == "Basic"]$Times
tv.extended <- TV[Cable == "Extended"]$Times
tv.pooled <- c(tv.basic, tv.extended)

observed <- mean(tv.basic) - mean(tv.extended)

mean(tv.basic); mean(tv.extended)
```

b.) Bootstrap the difference in mean times, plot the distribution, and give summary statistics of the bootstrap distribution. Obtain a 95% bootstrap percentile confidence interval, and interpret this interval.

```{r}
N <- 10e3; n <- length(TV$Times); alpha <- 0.05

bootstrap.values <- vector(mode = "numeric", N)

for(i in 1:N)
{
   samp.basic <- sample(tv.basic, size = length(tv.basic), replace = T)
   samp.extended <- sample(tv.extended, size = length(tv.extended), replace = T)
   
   bootstrap.values[i] <- mean(samp.basic) - mean(samp.extended)
}

boot.mean <- mean(bootstrap.values)
boot.se <- sd(bootstrap.values)
boot.bias <- boot.mean - observed

quantile(bootstrap.values, c(alpha/2, 1 - alpha/2)) # bootstrap interval
```

The 95% boostrap interval does not contain 0, suggesting there is in fact a difference in the times between basic and extended (basic being longer).

c.) What is the bootstrap estimate of the bias?

```{r}
boot.bias

boot.bias / boot.se # about 1% of the standard error
```

d.) Conduct a permutation test to see if the difference in mean commercial times is statistically significant, and state your conclusion.

```{r}
permutation.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp <- sample(length(tv.pooled), length(tv.basic), replace = F)
   
   samp.basic <- tv.pooled[samp]; samp.extended <- tv.pooled[-samp]
   
   permutation.values[i] <- mean(samp.basic) - mean(samp.extended)
}

ggplot(data.table(values = permutation.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_vline(xintercept = observed, col = "darkred")

p <- ( sum(permutation.values >= observed) + 1) / ( N + 1)
var <- p*(1 - p)/N
```

Permutation test supports the alternative hypothesis that there is a difference in commercial times.

### 5.16

Researchers conducted a study of primary and early secondary school children in Italy to examine gender differences in math anxiety. One of the measures used to understand math anxiety is the _Abbreviated Math Anxiety Scale (AMAS)_, a self-reported math anxiety questionnaire. A higher score indicates more max anxiety. The data set MathAnxiety contains the results for a subset of the children in the original study.

```{r}
MathAnxiety <- data.table(read.csv(paste0(data.dir, "MathAnxiety.csv"),
                                 header = T))
```

a.) Perform some exploratory analysis and obtain summary statistics of the AMAS scores for the boys and girls (do seperate analysis for each gender).

```{r}
ggplot(MathAnxiety, aes(Grade, AMAS)) +
   geom_boxplot(aes(fill = Grade))

ggplot(MathAnxiety, aes(Age, AMAS)) +
   geom_point()

ggplot(MathAnxiety, aes(RCMAS, AMAS)) +
   geom_point(aes(color = Gender))

ggplot(MathAnxiety, aes(Gender, AMAS)) +
   geom_boxplot(aes(fill = Gender))

ggpairs(MathAnxiety)
```

b.) Bootstrap the difference in mean times, plot the distribution, and give summary statistics of the bootstrap distribution. Obtain a 95% confidence interval, and interperet this interval.

```{r}
amas_boy <- MathAnxiety[Gender == "Boy"]$AMAS
amas_girl <- MathAnxiety[Gender == "Girl"]$AMAS

N <- 10e3; n <- nrow(MathAnxiety); alpha <- 0.05

observed <- mean(amas_boy) - mean(amas_girl)

bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp_boy <- sample(amas_boy, length(amas_boy), replace = T)
   samp_girl <- sample(amas_girl, length(amas_girl), replace = T)
   
   bootstrap.values[i] <- mean(samp_boy) - mean(samp_girl)
}

boot.mean <- mean(bootstrap.values)
boot.bias <- boot.mean - observed
boot.se <- sd(bootstrap.values)

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(color = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

boot.bias; boot.se

quantile(bootstrap.values, c(alpha/2, 1 - alpha/2)) # conf interval
```

The 95% confidence interval is -2.8 ~ -0.7, which does not include zero. The data suggests that there is a statistical difference between the self-reported AMAS scores between boys and girls (boys being lower).

c.) What is the bootstrap estimate of the bias? What fraction of the bootstrap; standard error does this represent?

```{r}
boot.bias

boot.bias / boot.se
```

Bootstrap bias is .006, which is less than 1% of the se.

d.) Conduct a permutation test to see if the difference in mean AMAS scores is statistically significant, and state your conclustion.

```{r}
amas_pooled <- c(amas_boy, amas_girl)

n <- length(amas_pooled); observed <- mean(amas_boy) - mean(amas_girl)

permutation.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp <- sample(1:n, length(samp_boy), replace = F)
   
   samp_boys <- amas_pooled[samp]; samp_girl <- amas_pooled[-samp]
   
   permutation.values[i] <- mean(samp_boy) - mean(samp_girl)
}

mean(permutation.values)

ggplot(data.table(value = permutation.values), aes(value)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed)

p <- ( sum(permutation.values <= observed) + 1 ) / (N + 1)
var <- p*(1 - p)/ N

t.test(amas_boy, amas_girl, alternative = "less")
```

The p-value for the permutation test is less than 1%, we reject the null hypothesis that there is no difference in self-reported AMAS scores between boys and girls. This conclustion is in-line with our results from the bootstraped difference in mean test.

### 5.17

Import the data from Girls2004.

```{r}
Girls2004 <- data.table(read.csv(paste0(data.dir, "Girls2004.csv"),
                                 header = T))
```

a.) Perform some exploratory data analysis, and obtain summary statistics on the weights of baby girls born in Wyoming and Alaska.

```{r}
ggpairs(Girls2004)

ggplot(Girls2004, aes(Smoker, Weight, fill = State)) +
   geom_boxplot() +
   scale_y_continuous(labels = scales::comma)

ggplot(Girls2004, aes(MothersAge, Weight, fill = State)) +
   geom_boxplot()
```

b.) Bootstrap the difference in means, plot the distribution, and give the summary statistics. Obtain a 95% confidence boostrap percentile confidence interval and interpret this interval.

```{r}
weights_wy <- Girls2004[State == "WY"]$Weight
weights_ak <- Girls2004[State == "AK"]$Weight

observed <- mean(weights_wy) - mean(weights_ak); N <- 10e3
alpha <- 0.05

bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp_wy <- sample(weights_wy, size = length(weights_wy), replace = T)
   samp_ak <- sample(weights_ak, size = length(weights_ak), replace = T)

   bootstrap.values[i] <- mean(samp_wy) - mean(samp_ak)
}

bootstrap.mean <- mean(bootstrap.values)
bootstrap.bias <- bootstrap.mean - observed
bootstrap.se <- sd(bootstrap.values)

ggplot(data.table(value = bootstrap.values), aes(value)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")


quantile(bootstrap.values, c(Lower = alpha/2, Upper = 1- alpha/2))
```

There is a statistically significant difference in the mean birth weights of girls born in Wyoming and Alaska (Wyoming girls weighing less on average), by between 530 and 96 oz less.

c.) What is the bootstrap estimate of the bias? What fraction of thh boostrap standard error does it represent?

```{r}
bootstrap.bias

bootstrap.bias / bootstrap.se
```

d.) Conduct a permutation test to see if the difference in mean weights is statistically significant, and state your conclusion.

```{r}
observed <- mean(weights_wy) - mean(weights_ak)
weights_pooled <- c(weights_wy, weights_ak)

permutation.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   samp <- sample(length(weights_pooled), size = length(weights_ak), replace = F)
   
   samp_wy <- weights_pooled[samp]; samp_ak <- weights_pooled[-samp]
   
   permutation.values[i] <- mean(samp_wy) - mean(samp_ak)
}

mean(permutation.values)

ggplot(data.table(values = permutation.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

p <- (sum(permutation.values <= observed) + 1) / (N + 1)

var <- p*(1 - p)/N
```

The permutation test further supports the claim that the weights of baby girls born in Wyoming and Alaska are different. We reject the null hypothesis that the means are the same at the 0.05% level (p = 0.003).

e.) For what population(s), if any, does this conclusion hold?

_I do not
### 5.18

Is there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set _Groceries_ contain, a sample of grocery items and their prices advertised on their respective websites on one specific day.

```{r}
Groceries <- data.table(read.csv(paste0(data.dir, "Groceries.csv"),
                                 header = T))
```

a.) Compute summary statistics of the prices for each store.

```{r, fig.height=7}
groceries <- melt(Groceries %>% select(-Size), 
                  id.vars = c("Product"), 
                  value.name = "Price", 
                  variable.name = "Store")

setorder(groceries, -Product)

ggplot(groceries, aes(Product, Price, col = Store, group = Store)) +
   geom_point(aes(color = Store)) +
   coord_flip()
```

b.) Use the bootstrap to determine whether or not there is a difference in the mean prices.

```{r}
prices.target <- groceries[Store == "Target"]$Price
prices.walmart <- groceries[Store == "Walmart"]$Price

N <- 10e3

observed <- mean(prices.target) - mean(prices.walmart)
bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   index <- sample(length(prices.target), size = length(prices.target), replace = T)
   
   samp.target <- prices.target[index]
   samp.walmart <- prices.walmart[index]
   
   bootstrap.values[i] <- mean(samp.target) - mean(samp.walmart)
}

boot.mean <- mean(bootstrap.values)
boot.bias <- boot.mean - observed
boot.se <- sd(bootstrap.values)

boot.bias / boot.se

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

quantile(bootstrap.values, c(Lower = alpha/2, Upper = 1 - alpha/2))
```

There doesn't appear to be a statistically significant difference in the average prices at the stores.

c.) Create a histogram of the difference in prices. What is unusal about Quaker Oats Life cereal?

```{r}
Groceries[Product == "Quaker Oats Life Cereal  Original ",]
```

It's almost double the price.

d.) Recompute the bootstrap percentile interval without this observation. What do you conclude?

```{r}
groceries_sans_quaker <- groceries[Product != "Quaker Oats Life Cereal  Original ",]

stopifnot(nrow(groceries_sans_quaker) == nrow(groceries) - 2) # ensure removeed

prices.target <- groceries_sans_quaker[Store == "Target"]$Price
prices.walmart <- groceries_sans_quaker[Store == "Walmart"]$Price

N <- 10e3

observed <- mean(prices.target) - mean(prices.walmart)
bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   # paired data, must use the same products from each company
   index <- sample(length(prices.target), size = length(prices.target), replace = T)
   
   samp.target <- prices.target[index]
   samp.walmart <- prices.walmart[index]
   
   bootstrap.values[i] <- mean(samp.target) - mean(samp.walmart)
}

boot.mean <- mean(bootstrap.values)
boot.bias <- boot.mean - observed
boot.se <- sd(bootstrap.values)

boot.bias / boot.se
ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

quantile(bootstrap.values, c(Lower = alpha/2, Upper = 1 - alpha/2))
```

The bootstrap confidence intervals suggest that there is a statistically significant difference in prices (Target being more expensive than Walmart). The Quaker Oats cereal made a difference.

### 5.19

Do chocolate and vanilla ice cream have the same number of calories?

The data set Ice Cream contains calorie information for a sample of brands of chocolate and vanilla ice cream.

```{r}
IceCream <- data.table(read.csv(paste0(data.dir, "IceCream.csv"),
                                 header = T))
```

a.) Compute summary statistics of the calories for the two flavors.

```{r}
icecream <- IceCream %>%
   mutate(Vanilla = VanillaCalories,
          Chocolate = ChocolateCalories) %>%
   select(Vanilla, Chocolate) %>%
   melt(variable.name = "Flavor", value.name = "Calories") %>%
   as.data.table()

stopifnot(nrow(icecream) == nrow(IceCream) * 2)

ggplot(icecream, aes(Calories, group = Flavor)) +
   geom_density(aes(fill = Flavor), alpha = .35)

ggplot(icecream, aes(Flavor, Calories, fill = Flavor)) +
   geom_boxplot()
```

b.) Use the bootstrap to determine whether or not there is a difference in the mean number of calories.

```{r}
calories.vanilla <- icecream[Flavor == "Vanilla"]$Calories
calories.chocolate <- icecream[Flavor == "Chocolate"]$Calories

N <- 10e3

observed <- mean(calories.chocolate) - mean(calories.vanilla)

bootstrap.values <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   # paired data, must use the same samples from chocolate and vanilla (ie, same manufacture)
   index <- sample(length(calories.vanilla), length(calories.vanilla), replace = T)
   
   v.samp <- calories.vanilla[index]
   c.samp <- calories.chocolate[index]
   
   bootstrap.values[i] <- mean(c.samp) - mean(v.samp)
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed)

bootstrap.mean <- mean(bootstrap.values)
bootstrap.bias <- bootstrap.mean - observed
bootstrap.se <- sd(bootstrap.values)

bootstrap.bias / bootstrap.se

quantile(bootstrap.values, c(Lower = alpha/2, Upper = 1 - alpha/2))
```

There appers to be a statistically difference in the amount of calories (Chocolate having more, by approx 4-12 on average).

### 5.20

In a remark at the end of Section 5.4.1, we mentioned that the procedure for bootstrapping the difference of medians is different than for thee mean.

Import the data set Diving2017.

```{r}
Diving2017 <- data.table(read.csv(paste0(data.dir, "Diving2017.csv"),
                                 header = T))
```

a.) Compute the difference in the median scores in the final and semi-final rounds.

```{r}
observed <- median(Diving2017$Final) - median(Diving2017$Semifinal)
```

b.) Calculate a 95% bootstrap interval for this statistic.

```{r}
N <- 10^5

result <- vector(mode = "numeric", length = N)

for(i in 1:N)
{
   index <- sample(nrow(Diving2017), replace = T)
   
   Dive.boot <- Diving2017[index, ] # resample pairs
   result[i] <- median(Dive.boot$Final) - median(Dive.boot$Semifinal)
}

ggplot(data.table(values = result), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange")
```

### 5.21

Two college students collecteed data on the price of hardcover textbooks from two disciplinary areas: Mathematics and the Natural Sciences and the Social Sciences. The data are in the file BookPrices.

```{r}
BookPrices <- data.table(read.csv(paste0(data.dir, "BookPrices.csv"),
                                 header = T))
```

a.) Perform some exploratory data analysis on book prices for each of the two diciplinary areas.

```{r}
ggpairs(BookPrices)

ggplot(BookPrices, aes(Area, Price, group = Area)) +
   geom_boxplot(aes(fill = Area))
```

b.) Bootstrap the mean of book price for each area separately, and describe the distributions.

```{r}

prices.math <- BookPrices[Area == "Math & Science"]$Price
prices.science <- BookPrices[Area == "Social Sciences"]$Price

N <- 10e4

bootstrap.values <- vector(mode = "numeric", length = N); n <- length(prices.math)
observed <- mean(prices.math)

for(i in 1:N)
{
   bootstrap.values[i] <- mean( sample(prices.math, n, replace = T) )
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

bootstrap.mean <- mean(bootstrap.values)

```

```{r}
N <- 10e4

bootstrap.values <- vector(mode = "numeric", length = N); n <- length(prices.science)
observed <- mean(prices.science)

for(i in 1:N)
{
   bootstrap.values[i] <- mean( sample(prices.science, n, replace = T) )
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

bootstrap.mean <- mean(bootstrap.values)
```

_Both of the distributions are relatively symetrical and approximately normal._

c.) Bootstrap the ratio of means. Provide plots of the bootstrap distribution and comment.

```{r}
N <- 10e4

bootstrap.values <- vector(mode = "numeric", length = N); n <- length(prices.science)
observed <- mean(prices.math) / mean(prices.science)

for(i in 1:N)
{
   samp.math <- sample(prices.math, length(prices.math), replace = T)
   samp.science <- sample(prices.science, length(prices.science), replace = T)
   
   bootstrap.values[i] <- mean(samp.math) / mean(samp.science)
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed, col = "darkred")

bootstrap.mean <- mean(bootstrap.values)
```

_The ratio of means distribution is heavly skewed with a long right tail._

d.) Find the 95% bootstrap percentile interval for the ratio of means. Interpret this interval.

```{r}
alpha <- 0.05

quantile(bootstrap.values, c(alpha/2, 1 - alpha/2))
```

_There is statistically significant evidence that the cost of math books are higher than science books._

e.) What is the bootstrap estimate of the bias? What fraction of the bootstrap standard error does it represent?

```{r}
bootstrap.bias <- mean(bootstrap.values) - observed
bootstrap.se <- sd(bootstrap.values)

bootstrap.bias

bootstrap.bias / bootstrap.se
```

The bias in the bootstrap is quite high at 5%. Also, the bias is almost 16% of the SE which is extremely high.

### 5.22

Import the data from flight delays case study in S1.1.

For this study, we will consider the ratio of means.

```{r}
FlightDelays <- data.table(read.csv(paste0(data.dir, "FlightDelays.csv"),
                                 header = T))
```

a.) Perform some exploratory data analysis on flight delay lengths.

```{r}
ggpairs(FlightDelays)

ggplot(FlightDelays, aes(Carrier, Delay, col = Carrier)) +
   geom_boxplot()
```

b.) Bootstrap the mean of flight delay lengths for each airline separately, and describe the distribution.

```{r}
ua.delay <- FlightDelays[Carrier == "UA"]$Delay
aa.delay <- FlightDelays[Carrier == "AA"]$Delay

n <- length(ua.delay); N <- 10e4
bootstrap.values <- vector(mode = "numeric", length = n)

for(i in 1:N)
{
   bootstrap.values[i] <- mean( sample(ua.delay, n, replace = T))   
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange")


n <- length(aa.delay); N <- 10e4
bootstrap.values <- vector(mode = "numeric", length = n)

for(i in 1:N)
{
   bootstrap.values[i] <- mean( sample(aa.delay, n, replace = T))   
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange")

```

c.) Bootstrap the ratio of means. Provide plots of the bootstrap distribution and describe the distribution.

```{r}
n <- length(aa.delay); N <- 10e4
bootstrap.values <- vector(mode = "numeric", length = n)

observed <- mean(aa.delay) / mean(ua.delay)

for(i in 1:N)
{
   bootstrap.values[i] <- mean( sample(aa.delay, n, replace = T)) / mean( sample(ua.delay, n, replace = T))   
}

ggplot(data.table(values = bootstrap.values), aes(values)) +
   geom_histogram(aes(y = ..density.., fill = ..count..), bins = 30) +
   geom_density(col = "darkorange") +
   geom_vline(xintercept = observed)
```

d.) Find and interpret the 95% confidence bootstrap interval for the ratio of means. 

```{r}
quantile(bootstrap.values, c(alpha/2, 1 - alpha/2))
```

Distribution is approximately normal, and the interval contains zero so we can't rule out chance in the flight delay data.

e.) What is the bootstrap estimate of the bias? What fraction of the standard error does it represent?

```{r}
boot.bias <- mean(bootstrap.values) - observed

boot.bias / sd(bootstrap.values)
```

The se is approximately 3% of the bias.