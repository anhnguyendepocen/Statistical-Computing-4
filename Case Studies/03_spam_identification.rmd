---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Spam Identification}
   \rfoot{\color{headergrey}Chapter 3}
   \lfoot{\color{headergrey}}
   \fancyfoot[C]{\rmfamily\color{headergrey}Case Studies In Data Science}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}

knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 4.5)

options(knitr.table.format = "latex")

```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)

library(here, quietly = TRUE, warn.conflicts = FALSE)

library(codetools, quietly = TRUE, warn.conflicts = FALSE)
library(lattice, quietly = TRUE, warn.conflicts = FALSE)
library(fields, quietly = TRUE, warn.conflicts = FALSE)
library(RColorBrewer, quietly = TRUE, warn.conflicts = FALSE)
library(tm, quietly = TRUE, warn.conflicts = FALSE)

library(rbenchmark, quietly = TRUE, warn.conflicts = FALSE)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- file.path(here::here(), "Case Studies", "datasets", "spam")

```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

# Using Statistics to Identify Spam

## Anatomy of an email Message

### Spam Data

```{r}

head(list.files(path = file.path(data.dir, "easy_ham")))
head(list.files(path = file.path(data.dir, "spam_2")))

directories <- paste(data.dir, list.files(data.dir), sep = .Platform$file.sep)

file_counts <- sapply(directories, function(dir) length(list.files(dir)))

total_files <- sum(file_counts)
total_files
```

```{r}
file_counts

idx <- c(1:5, 15, 27, 68, 69, 329, 404, 427, 516, 852, 971)

fn <- list.files(directories[1], full.names = T)[idx]

sampleEmail <- sapply(fn, readLines)
```

### Text Mining and Naive Bayes Classification

```{r}
msg <- sampleEmail[[1]]
which(msg == "")[1]

match("", msg)

splitPoint <- match("", msg)

msg[ (splitPoint - 2):(splitPoint + 6)]

header <- msg[1:(splitPoint - 1)]
body <- msg[ -(1:splitPoint) ]

```


```{r}

splitMessage <- function(msg) {
   splitPoint <- match("", msg)
   
   header <- msg[ 1:(splitPoint - 1)]
   body <- msg[ -(1:splitPoint)]
   
   return(list(header = header, body = body))
}

sampleSplit <- lapply(sampleEmail, splitMessage)

header <- sampleSplit[[1]]$header
grep("Content-Type", header)
grep("multi", tolower(header))

header[46]

headerList <- lapply(sampleSplit, function(msg) msg$header)

CTloc <- sapply(headerList, grep, pattern = "Content-Type")
CTloc

sapply(headerList, function(header) {
   CTloc <- grep("Content-Type", header)
   if( length(CTloc) == 0) return(NA)
   CTloc
})

hasAttach <- sapply(headerList, function(header) {
   CTloc <- grep("Content-Type", header)
   
   if(length(CTloc) == 0) return(F)
   grepl("multi", tolower(header[CTloc]))
})

hasAttach

header <- sampleSplit[[6]]$header
boundaryIdx <- grep("boundary=", header)
header[boundaryIdx]

sub(".*boundary=\"(.*)\";.*", "\\1", header[boundaryIdx])

header2 <- headerList[[9]]
boundaryIdx2 <- grep("boundary=", header2)
header2[boundaryIdx2]

sub('.*boundary="(.*)";.*', "\\1", header2[boundaryIdx2])

boundary2 <- gsub('"', "", header2[boundaryIdx2])

sub(".*boundary= *(.*);?.*", "\\1", boundary2)

boundary <- gsub('"', "", header[boundaryIdx])
sub(".*boundary= *(.*);?.*", "\\1", boundary)

```

```{r}

getBoundary <- function(header) {
   boundaryIdx <- grep("boundary=", header)
   boundary = gsub('"', "", header[boundaryIdx])
   gsub(".*boundary= *([^;]*);?.*", "\\1", boundary)
}

```

```{r}

boundary <- getBoundary(headerList[[15]])
body <- sampleSplit[[15]]$body

bString <- paste("--", boundary, sep = "")
bStringLocs <- which(bString == body)
bStringLocs

eString <- paste("--", boundary, "--", sep = "")
eStringLoc <- which(eString == body)
eStringLoc

msg <- body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)]
tail(msg)

msg <- c(msg, body[ (eStringLoc + 1) : length(body) ])
tail(msg)

```

### Handle Attachments

```{r}


```

### Extracting Words from the Message Body

```{r}
head(sampleSplit[[1]]$body)

msg <- sampleSplit[[3]]$body
head(msg)

```

### Stemming

```{r}
exclude_word_list <- stopwords(kind = "en")
```

### Convert To Wordlist

```{r}
tolower(gsub("[[:punct:]0-9[:blank:]]+", " ", msg))

msg[ c(1, 3, 26, 27) ]

cleanMsg <- tolower(gsub("[[:punct:]0-9[:blank:]]+", " ", msg))
cleanMsg[ c(1, 3, 26, 27) ]

words <- unlist(strsplit(cleanMsg, "[[:blank:]]+"))

words <- words[ nchar(words) > 1 ]

words <- words[ ! (words %in% exclude_word_list) ]

head(words)

```

```{r}

findMsgWords <- function(msg, exclude) {
   
   cleanMsg <- tolower(gsub("[[:punct:]0-9[:blank:]]+", " ", msg))

   words <- unlist(strsplit(cleanMsg, "[[:blank:]]+"))
   
   keep <- sapply(words, function(word) return(!(word %in% exclude)))
   
   
   return(words[ keep ])   
}

```

### Prep Wrap-Up

```{r}

dropAttach <- function(body, boundary) {
   
   if(is.null(body)) {
      return("")
   }
   
   bString <- paste("--", boundary, sep = "")
   bStringLocs <- which(bString == body)
   
   eString <- paste("--", boundary, "--", sep = "")
   eStringLoc <- which(eString == body)
   
   if(length(bStringLocs) == 2) {
      msg <- body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)]
   }
   
   if(length(eStringLoc) > 0) {
      msg <- c(msg, body[ (eStringLoc + 1) : length(body) ])
   }
   
   return(msg)
}

processAllWords <- function(dirName, stopWords) {
   # read all files in the directory
   fileNames <- list.files(dirName, full.names = T)
   
   # drop files that are not email, i.e., cmds
   notEmail <- grep("cmds$", fileNames)
   
   if( length(notEmail) > 0) fileNames <- fileNames[ -notEmail ]
   
   messages <- lapply(fileNames, readLines, encoding = "latin1")
   
   # split header and body
   emailSplit <- lapply(messages, splitMessage)
   
   # put body and header in own lists
   bodyList <- lapply(emailSplit, function(msg) msg$body)
   headerList <- lapply(emailSplit, function(msg) msg$header)
   rm(emailSplit)
   
   # determine which messages have attachments
   hasAttach <- sapply(headerList, function(header) {
      
      CTloc <- grep("Content-Type", header)
      
      if( length(CTloc) == 0) return(0)
      
      multi <- grep("multi", tolower(header[CTloc]))
      
      if( length(multi) == 0 ) return(0)
      
      multi
   })
   
   hasAttach <- which(hasAttach > 0)
   
   # find boundary string for messages with attachments
   boundaries <- sapply(headerList[hasAttach], getBoundary)
   
   # drop attachments from message body
   bodyList[hasAttach] <- mapply(dropAttach, bodyList[hasAttach],
                                 boundaries, SIMPLIFY = F)
   
   # extract words from body
   msgWordsList <- lapply(bodyList, findMsgWords, stopWords)
   
   invisible(msgWordsList)
}
```

### Build Email Database

```{r}

msgWordList <- lapply(directories, processAllWords, stopWords = exclude_word_list)

numMsgs <- sapply(msgWordList, length)
numMsgs

isSpam <- rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)

msgWordsList <- unlist(msgWordList, recursive = F)

```

## Naive Bayes Classifier Implementation

### Train / Test Split

```{r}
numEmail <- length(isSpam)

numSpam <- sum(isSpam)
numHam <- numEmail - numSpam

set.seed(418910)

testSpamIdx <- sample(numSpam, size = floor(numSpam/3))
testHamIdx <- sample(numHam, size = floor(numHam/3))

testMsgWords <- c((msgWordsList[isSpam])[testSpamIdx],
                  (msgWordsList[!isSpam])[testHamIdx])

trainMsgWords <- c((msgWordsList[isSpam])[ - testSpamIdx ],
                   (msgWordsList[!isSpam])[ - testHamIdx])

testIsSpam <- rep(c(T, F), 
                  c(length(testSpamIdx), length(testHamIdx)))

trainIsSpam <- rep(c(T, F),
                   c(numSpam - length(testSpamIdx),
                     numHam - length(testHamIdx)))

```

### Probability Estimates from Training Sample

```{r}
bow <- unique(unlist(trainMsgWords))

length(bow)
```

```{r}
spamWordCounts <- rep(0, length(bow))

names(spamWordCounts) = bow

tmp <- lapply(trainMsgWords[trainIsSpam], unique)
tt <- table( unlist(tmp) )
spamWordCounts[ names(tt) ] = tt

spamWordsProbs <- (spamWordCounts + 0.5) / (sum(trainIsSpam) + 0.5)

spamWordsProbs[1:20]

hamWordCounts <- rep(0, length(bow))

names(hamWordCounts) = bow

tmp <- lapply(trainMsgWords[ - trainIsSpam], unique)
tt <- table( unlist(tmp) )
hamWordCounts[ names(tt) ] = tt

hamWordsProbs <- (hamWordCounts + 0.5) / (sum(!trainIsSpam) + 0.5)

log(spamWordsProbs) - log(hamWordsProbs)

```

```{r}
wordsList <- trainMsgWords
spam <- trainIsSpam

make_words_valid_columns <- function( words, all_words ) {
   
   word_counts <- rep(0, length(all_words))
   names(word_counts) <- all_words
   
   tmp <- lapply(words, unique)
   tt <- table( unlist(tmp) )
   word_counts[ names(tt) ] = tt

   return(word_counts)
}

computeFreqs <- function(wordsList, spam, bow = unique(unlist(wordsList))) {
   
   all_words <- unique(bow)
   
   # create a matrix for spam, ham, and log odds
   wordTable <- matrix(0.5, nrow = 2, ncol = length(bow))
   colnames(wordTable) <- all_words
   rownames(wordTable) <- c( "presentLogOdds",
                          "absentLogOdds")
   
   # for each spam message, add 1 to the counts for words in messsage
   
   spam_all <- wordsList[spam]
   spam_words <- make_words_valid_columns( spam_all, all_words )

   wordTable <- rbind(wordTable, spam_words + 0.5)
   rownames(wordTable)[3] <- "spam"
   
   # Similarly for ham messages
   
   ham_all <- wordsList[ !spam ]
   
   ham_words <- make_words_valid_columns( ham_all, all_words )
   
   wordTable <- rbind(wordTable, ham_words + 0.5)
   rownames(wordTable)[4] <- "ham"
   
   head(wordTable[, 1:20])
   
   # find the total number of spam and ham
   numSpam <- sum(spam)
   numHam <- length(spam) - numSpam
   
   # prob (word|spam) and prob(words|ham)
   wordTable["spam", ] <- wordTable["spam", ] / (numSpam + 0.5)
   wordTable["ham", ] <- wordTable["ham", ] / (numHam + 0.5)
   
   head(wordTable[, 1:20])
   
   # log odds
   wordTable["presentLogOdds", ] = 
      log(wordTable["spam", ]) - log(wordTable["ham", ])
   
   wordTable["absentLogOdds", ] =
      log((1 - wordTable["spam", ])) - log((1 - wordTable["ham", ]))
   
   invisible(wordTable)
}

```

```{r}
trainTable <- computeFreqs(trainMsgWords, trainIsSpam)

# peek the prob table
head(trainTable[, 1:10])
```

### Classifying New Messages

```{r}
newMsg <- testMsgWords[[1]]

# only look at words we have classified
newMsg <- newMsg[ !is.na(match(newMsg, colnames(trainTable)))]

present <- colnames(trainTable) %in% newMsg

sum( trainTable["presentLogOdds", present]) +
   sum( trainTable["absentLogOdds", !present])

newMsg <- testMsgWords[[ which(!testIsSpam)[ 1 ] ]]
newMsg <- newMsg[ !is.na(match(newMsg, colnames(trainTable)))]
present <- (colnames(trainTable) %in% newMsg)

sum(trainTable["presentLogOdds", present]) +
   sum(trainTable["absentLogOdds", !present])

```

