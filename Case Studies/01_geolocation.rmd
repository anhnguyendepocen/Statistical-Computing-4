---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Geolocation}
   \rfoot{\color{headergrey}Chapter 1}
   \lfoot{\color{headergrey}}
   \fancyfoot[C]{\rmfamily\color{headergrey}Case Studies In Data Science}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}

knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")

```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)

library(here, quietly = TRUE, warn.conflicts = FALSE)

library(codetools, quietly = TRUE, warn.conflicts = FALSE)
library(lattice, quietly = TRUE, warn.conflicts = FALSE)
library(fields, quietly = TRUE, warn.conflicts = FALSE)

library(rbenchmark, quietly = TRUE, warn.conflicts = FALSE)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- file.path(here::here(), "Case Studies", "datasets")

```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

# Geolocation

## Predicting Location via Indoor Positioning Systems

### The Raw Data

```{r}

file_offline <- file.path(data.dir, "offline.final.trace.txt")
file_online <- file.path(data.dir, "online.final.trace.txt")

raw_offline <- read_lines(file_offline)
raw_online <- read_lines(file_online)

```

#### Sanity Check

```{r}
# number of comment lines in the data
sum(substr(raw_offline, 1, 1) == "#")

# total number of lines in the data
length(raw_offline)
```

#### Data Pre-processing

Generate read data function for preprocessing training and test data.

```{r}

processLine <- function(x) {
   tokens <- strsplit(x, "[;=,]")[[1]]
   
   if(length(tokens) == 10)
      return(NULL)

   tmp <- matrix(tokens[ - (1:10)], ncol = 4, byrow = T)
   cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),
                ncol = 6, byrow = T), tmp)
}

validLines <- function(data) {
   substr(data, 1, 1) != "#"
}

roundOrientation <- function(angles) {
   refs = seq(0, by = 45, length = 9)
   q <- sapply(angles, function(o) which.min(abs(o - refs)))
   c(refs[1:8], 0)[q]
}

readData <- function(file, submacs = macs) {
   
   lines <- read_lines(file)
   
   valid_lines <- lines[ validLines(lines) ]
   
   processed_lines <- lapply(valid_lines, processLine)
   
   data <- as.data.table(do.call("rbind", processed_lines),
                         stringsAsFactors = F)

   names(data) <- c("time", "scanMac", "posX", "posY", "posZ",
                       "orientation", "mac", "signal", 
                       "channel", "type")
   
   numVars <- c("time", "posX", "posY", "posZ",
                "orientation", "signal")

   data[, (numVars) := lapply(.SD, as.numeric), .SDcols = numVars]

   data <- data[ data$type == 3, ]
   data[, type := NULL]
   
   data[, rawTime := time]
   data[, time := time/1000]
   class(data$time) = c("POSIXt", "POSIXct")
   
   # drop scanMac & posZ
   data[, `:=`(scanMac = NULL, posZ = NULL)]
   
   data$angle = roundOrientation(data$orientation)
   
   data$channel = NULL
   
   data$posXY <- paste(data$posX, data$posY, sep = "-")
   
   return(data[mac %in% submacs])
}

```

#### Test Pre-processor

```{r}
lines <- processLine(raw_offline[4:20])
lines

offline_valid <- raw_offline[validLines(raw_offline)]

head(offline_valid)

length(offline_valid)
```

```{r}
tmp <- lapply(offline_valid[1:17], processLine)

sapply(tmp, nrow)
```

#### Dry Run

```{r}
offline_test <- as.data.table(do.call("rbind", tmp))

offline_test
```

#### Process

```{r}
# Enter Debug Context

# options(error = recover, warn = 2)

offline_data <- lapply(offline_valid, processLine)
offline <- as.data.table(do.call("rbind", offline_data),
                         stringsAsFactors = F)

names(offline) <- c("time", "scanMac", "posX", "posY", "posZ",
                    "orientation", "mac", "signal", 
                    "channel", "type")

numVars <- c("time", "posX", "posY", "posZ",
             "orientation", "signal")

dim(offline)

offline[, (numVars) := lapply(.SD, as.numeric), .SDcols = numVars]

offline <- offline[ offline$type == 3, ]
offline[, type := NULL]

offline[, rawTime := time]
offline[, time := time/1000]
class(offline$time) = c("POSIXt", "POSIXct")

# options(error = recover, warn = 1)
```

#### Data Cleaning

```{r}
summary(offline)
```

### Orientation Exploration

```{r}
length(unique(offline$orientation))
```

```{r, fig.height=5}
plot(ecdf(offline$orientation))
```

```{r}
ggplot(offline, aes(orientation)) +
   stat_ecdf() +
   labs("Orientation")
```

```{r}
ggplot(offline, aes(orientation, fill = ..count..)) +
   geom_histogram(bins = 30) +
   scale_y_continuous(labels = comma) +
   labs("Orientation Value Clusters")
```

```{r}

offline$angle <- roundOrientation(offline$orientation)

# angle = cleaned orientation column
ggplot(offline, aes(angle)) +
   stat_ecdf()

ggplot(offline, aes(angle, fill = ..count..)) +
   geom_histogram(bins = 30) +
   scale_y_continuous(labels = comma) +
   labs("Cleaned Up Angles")
```

```{r}
with(offline, boxplot(orientation ~ angle,
                      xlab = "nearest 45 degree angle",
                      ylab = "orientation"))

ggplot(offline, aes(angle, orientation, group = angle)) +
   geom_boxplot() +
   labs(title = "Cleaned Angle vs Raw Orientation", 
        x = "nearest 45 degree angle", 
        y = "orientation")
```

### MAC Address

```{r}
c(length(unique(offline$mac)), length(unique(offline$channel)))

table(offline$mac)

ggplot(offline, aes(mac, fill = ..count..)) +
   geom_histogram(stat = "count", bins = 30) +
   scale_y_continuous(labels = comma)

# keep only the top 7 MAC address data points

dim(offline)

offline_macs <- names(sort(table(offline$mac), decreasing = T)[1:7])

dim(offline)
```

```{r}
macChannel <- with(offline, table(mac, channel))
apply(macChannel, 1, function(x) sum(x > 0))

# mac and channel are 1:1, we can remove channel
#offline$channel := NULL
```

### Exploring the Position of the Hand-Held Device

```{r}
locDF <- with(offline,
              by(offline, list(posX, posY), function(x) x))

length(locDF)

sum(sapply(locDF, is.null))

locDF <- locDF[ !sapply(locDF, is.null)]

length(locDF)

locCounts <- sapply(locDF, nrow)

locCounts <- sapply(locDF,
                    function(df)
                       c(df[1, c("posX", "posY")], count = nrow(df)))

class(locCounts)

dim(locCounts)

locCounts[ , 1:8]

locCountsDF <- as.data.table(t(locCounts))

locCountsDF$posX <- unlist(locCountsDF$posX)
locCountsDF$posY <- unlist(locCountsDF$posY)

ggplot(locCountsDF, aes(posX, posY, label = count)) +
   geom_text(angle = 45) +
   labs(title = "X / Y postion counts")
```

### Final Data Prep

```{r}
offline <- readData(file_offline, offline_macs)
```

#### Signal Strength

```{r, fig.height=7}
bwplot(signal ~ factor(angle) | mac, data = offline,
       subset = posX == 2 & posY == 12 & 
          mac != "00:0f:a3:39:dd:cd",
       layout = c(2, 3))

summary(offline$signal)
```

```{r, fig.height=7}
densityplot(~ signal | mac + factor(angle), data = offline,
            subset = posX == 24 & posY == 4 &
               mac != "00:0f:a3:39:dd:cd",
            bw = 0.5, plot.points = F)
```

```{r, fig.width=5}
byLocAngleAP <- with(offline,
                     by(offline, list(posXY, angle, mac),
                        function(x) x))
signalSummary <-
   lapply(byLocAngleAP,
          function(oneLoc) {
             ans = oneLoc[1, ]
             ans$medSignal = median(oneLoc$signal)
             ans$avgSignal = mean(oneLoc$signal)
             ans$num = length(oneLoc$signal)
             ans$sdSignal = sd(oneLoc$signal)
             ans$iqrSignal = IQR(oneLoc$signal)
             ans
          })

offlineSummary <- do.call("rbind", signalSummary)

breaks <- seq(-90, -30, by = 5)

bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),
       data = offlineSummary,
       subset = mac != "00:0f:a3:39:dd:cd",
       xlab = "Mean Signal", ylab = "SD Signal")
```

```{r}
with(offlineSummary,
     smoothScatter((avgSignal - medSignal) ~ num,
                   xlab = "Number of Observations",
                   ylab = "mean - median"))
abline(h = 0, col = "#984ea3", lwd = 2)

lo.obj <- with(offlineSummary,
               loess(diff ~ num,
                     data = data.frame(diff = (avgSignal - medSignal),
                                       num = num)))

lo.obj.pr <- predict(lo.obj, newdata = data.frame(num = (70:120)))
lines(x = 70:120, y = lo.obj.pr, col = "#4daf4a", lwd = 2)
```

#### Signal and Distance

```{r, fig.height=6}

subMacs <- names(sort(table(offline$mac), decreasing = T))[1:7]

surfaceSS <- function(data, mac, angle) {
   oneAPAngle <- subset(offlineSummary, mac == mac & angle == angle)   

   smothSS <- Tps(oneAPAngle[, c("posX", "posY")],
                  oneAPAngle$avgSignal)
   
   vizSmooth <- predictSurface(smothSS)
   
   plot.surface(vizSmooth, type = "C")
   points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)
}

surfaceSS(offlineSummary, subMacs[5], 0)
```

```{r, fig.height = 7}
parCur <- par(mfrow = c(2,2), mar = rep(1, 4))
mapply(surfaceSS, mac = subMacs[ rep(c(5, 1), each = 2)],
       data = list(data = offlineSummary))
par(parCur)
```

Exclude one of two similar access points

```{r}
offlineSummary <- subset(offlineSummary, mac != subMacs[2])
```

```{r}
AP <- matrix( c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,
                 1, 14, 33.5, 9.3, 33.5, 2.8),
              ncol = 2, byrow = T,
              dimnames = list(subMacs[ -2 ], c("x", "y") ))

ggplot(data.table(mac = rownames(AP), AP), aes(x, y)) +
   geom_point()
```

```{r, fig.height=7}
diffs <- offlineSummary[, c("posX", "posY")] - AP[offlineSummary$mac, ]

offlineSummary$dist <- sqrt(diffs[, 1]^2 + diffs[, 2]^2)

xyplot(signal ~ dist | factor(mac) + factor(angle),
       data = offlineSummary, pch = 19, cex = 0.3,
       xlab = "distance")
```

### Nearest Neighbor Methods to Predict Location

```{r}
macs <- unique(offlineSummary$mac)

online <- readData(file_online, submacs = macs)

length(unique(online$posXY))
```

```{r}
tabonlineXYA = table(online$posXY, online$angle)
tabonlineXYA[1:6, ]
```

```{r}
keepVars <- c("posXY", "posX", "posY", "orientation", "angle")

byLoc <- with(online,
              by(online, list(posXY),
                 function(x) {
                    ans <- x[1, ..keepVars]
                    avgSS <- tapply(x$signal, x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = 6,
                               dimnames = list(ans$posXY, names(avgSS)))
                    cbind(ans, y)
                 }))

onlineSummary <- do.call("rbind", byLoc)

dim(onlineSummary)

names(onlineSummary)
```

#### Choice of Orientation

```{r}

reshapeSS <- function(data, 
                      varSignal = "signal",
                      keepVars = c("posXY", "posX", "posY"),
                      sampleAngle = F) {
   
   if(sampleAngle)
      data <- data[angle == sample(data$angle, size = 1), ]
   
   byLocation <- with(data,
                 by(data, list(posXY),
                    function(x) {
                       ans <- x[1, ..keepVars]
                       avgSS <- tapply(x$signal, x$mac, mean)
                       y = matrix(avgSS, nrow = 1, dimnames = list(ans$posXY, names(avgSS)))
                       cbind(ans, y)
                    }))
   
   newDataSS <- do.call("rbind", byLocation)
   
   col_names <- colnames(newDataSS)
   to_change <- !(col_names %in% keepVars)
   
   n_cols <- length(col_names)
   start <- length(keepVars)
   
   colnames(newDataSS)[to_change] <- sapply(col_names[to_change], function(col) {
      n <- nchar(col)
      substr(col, n - 2, n)
   })
   
   newDataSS[, start:ncol(newDataSS)] <- round(newDataSS[, start:ncol(newDataSS)])
   
   return(newDataSS)
}

selectTrain <- function(angleNewObs, signals, m) {
   
   refs <- seq(0, by = 45, length = 8)
   
   nearestAngle <- roundOrientation(angleNewObs)
   
   if( m %% 2 == 1) {
      angles <- seq(-45 * (m - 1) / 2, 45 * (m - 1) / 2, length = m)
   } else {
      m = m + 1
      angles <- seq(-45 * (m - 1) / 2, 45 * (m - 1) / 2, length = m)
      
      if( sign(angleNewObs - nearestAngle) >= 1)
         angles = angles[ -1 ]
      else
         angles = angles[ -m ]
   }
   
   angles <- angles + nearestAngle
   angles[angles < 0] = angles[ angles < 0] + 360
   angles[angles > 360] = angles[ angles > 360 ] - 360
   
   signals <- signals[angle %in% angles, ]

   reshapeSS(signals, varSignal = "avgSignal")
}

findNN <- function(newSignal, trainSubset) {
   diffs <- apply(trainSubset[ , 4:9], 1,
                  function(x) x - newSignal)
   dists <- apply(diffs, 2, function(x) sqrt(sum(x^2)) )
   closest <- order(dists)
   return(trainSubset[closest, 1:3 ])
}

predXY <- function(newSignals, newAngles, trainData, 
                   numAngles = 1, k = 3) {
   
   closeXY <- list(length = nrow(newSignals))
   
   for(i in 1:nrow(newSignals)) {
      trainSS <- selectTrain(newAngles[i], trainData, m = numAngles)
      closeXY[[i]] <- findNN(newSignal = as.numeric(newSignals[i, ]),
                             trainSS)
   }
   
   estXY <- lapply(closeXY, function(x)
                              sapply(x[, 2:3],
                                     function(x) mean(x[1:k])))
   estXY <- do.call("rbind", estXY)
   
   return(estXY)
}

estXYk1 <- predXY(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 1)

estXYk3 <- predXY(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 3)
```

```{r}
calcError <- function(estXY, actualXY)
   sum( rowSums( (estXY - actualXY) ^ 2) )

actualXY <- onlineSummary[, c("posX", "posY")]
sapply(list(estXYk1, estXYk3), calcError, actualXY)

```

```{r}
v <- 11;

permuteLocs <- sample(unique(offlineSummary$posXY))
permuteLocs <- matrix(permuteLocs, ncol = v,
                      nrow = floor(length(permuteLocs)/v))

```

```{r}
onlineFold <- subset(offlineSummary, posXY %in% permuteLocs[, 1])

onlineCVSummary <- reshapeSS(offline, 
                              keepVars = c("posXY", "posX", "posY", "angle"),
                              sampleAngle = T)

offlineFold <- subset(offlineSummary,
                     posXY %in% permuteLocs[ , 1])

estFold <- predXY(newSignals = onlineFold[, 6:11],
                  newAngles = onlineFold[, 4],
                  offlineFold, numAngles = 3, k = 3)

actualFold <- onlineFold[, c("posX", "posY")]
calcError(estFold, actualFold)

```

```{r}
K <- 20
err <- rep(0, K)

for(j in 1:v) {
   onlineFold <- subset(onlineCVSummary,
                        posXY %in% permuteLocs[, j])
   offlineFold <- subset(offlineSummary,
                         posXY %in% permuteLocs[ , -j])
   
   actualFold <- onlineFold[, c("posX", "posY")]
   
   for(k in 1:K) {
      estFold <- predXY(newSignals = onlineFold[, 6:11],
                        newAngles = onlineFold[, 4],
                        offlineFold, numAngles = 3, k = k)
      
      err[k] <- err[k] + calcError(estFold, actualFold) 
   }
}


```


```{r}
k_values <- data.table(K = 1:K, Error = err, Min = which.min(err))

ggplot(k_values, aes(K, Error)) +
   geom_line(aes(col = Error)) +
   geom_hline(data = k_values[K == Min], aes(yintercept = Error), col = "darkblue") +
   labs("K-NN Training Error")
```

```{r}
estXYk5 <- predXY(newSignals = onlineSummary[, 6:11],
                 newAngles = onlineSummary[, 4],
                 offlineSummary, numAngles = 3, k = 5)
```

```{r}
calcError(estXYk5, actualXY)
```

```{r}
error_table <- data.table(K = c(1, 3, 5), 
                          Error = c(calcError(estXYk1, actualXY), 
                                    calcError(estXYk3, actualXY), 
                                    calcError(estXYk5, actualXY)))

ggplot(error_table, aes(K, Error, fill = -Error)) +
   geom_bar(stat = "identity")
```

### Further Analysis

### 1.)

Write the code to read the raw training data into the data structure in the first approach described in section 1.2. That is, the data structure is a data frame with a column for each MAC address that detected a signal. For the column name, use the last two characters of the MAC address, or some other unique identifier.

```{r}

file <- file_offline

lines <- read_lines(file)

valid_lines <- lines[ validLines(lines) ]

parseLine <- function(line) {
   
   pairs <- strsplit(line, ";")
   tokens <- lapply(pairs, function(x)strsplit(x, "="))
   
   mat <- matrix(unlist(tokens), ncol = 2, byrow = T)
   
   colnames <- mat[, 1]
   values <- mat[, 2]
   
   dt <- data.table(t(values))
   colnames(dt) <- colnames
   
   dt
}

rows <- lapply(valid_lines, parseLine)

offline_alt <- rbindlist(rows, fill = T)

head(offline_alt)
```

### 2.)

Compare the size of two data structures: the data frame created in Section 1.2 and the data frame created in the previous problem. 

+ Which uses less memory?

```{r}
object.size(offline)

object.size(offline_alt)
```

+ What are the dimensions of each?

```{r}
dim(offline)

dim(offline_alt)
```

+ How might this change with different numbers of devices in the building?

_Columns will expand out drastically with more devices._

+ Different number of signals from the less commonly detected devices?

_The more devices we have the more the columns in the alternative approach will be filled with NAs, thus bloating the object size significantly._

### 3.)

Compare the total time it takes to read the raw data and create the data frame, for the two approaches described in Section 1.2.

```{r}
file <- file_offline

fn_read_data <- function(file, processFun, sample = F, samp.size = 100 ) {
   start_time <- Sys.time()
   
   lines <- read_lines(file)
   valid_lines <- lines[ validLines(lines) ]
   
   n <- length(valid_lines)
   
   if(sample) {
      samp <- sample(n, samp.size, replace = F)
      
      valid_lines <- valid_lines[samp]
   }
   
   res <- processFun(valid_lines) # throw away
   
   return(Sys.time() - start_time)
}

fn_read_data(file_offline, processLine)

fn_read_data(file_offline, parseLine)
```

Do this for different size subsets of the data (chosen at random) and draw a curve of the time against input size for each of the approaches.

```{r}
sample_sizes <- seq( from = 100, to = 15000, by = 2000)

times_classic <- sapply(sample_sizes, function(samp) {
   fn_read_data(file_offline, processLine, sample = T, samp.size = samp)
})

times_alt <- sapply(sample_sizes, function(samp) {
   fn_read_data(file_offline, parseLine, sample = T, samp.size = samp)
})

results <- data.table(N = sample_sizes, Classic = times_classic, alt = times_alt)

ggplot(results) +
   geom_line(aes(N, Classic), col = "darkblue") +
   geom_line(aes(N, alt), col = "darkred")
```

Also, comment on the memory and speed for the two approaches.

_The alt approach is substantially worse in all aspects._

### 4.)

Examine the time variable in the offline data. Any chance over time in the characteristics of the signal caused by, e.g., reduced battery power in the measuring devices as time goes by, or measurments taken on different days may be made by different people with different levels of accuracy. Also, examination of time can give insights into how the experiment was carried out. 

Were the positions close to each other measured at similar times?

Do you see any change in the signal strength variable or mean over time?

Try controlling for other variables that might affect this relationship.

```{r}

offline_copy <- offline
offline_copy$ts <- strftime(offline_copy$time, format="%H:%M:%S")
offline_copy$ts_m <- strftime(offline_copy$time, format="%H:%M")
offline_copy$ts_h <- strftime(offline_copy$time, format="%H")

```

```{r}
ggplot(offline_copy, aes(ts, signal, fill = signal)) +
   geom_bar(stat = "identity") +
   scale_y_reverse()

```

```{r}
ggplot(offline_copy, aes(ts_m, signal, fill = signal)) +
   geom_bar(stat = "identity") +
   scale_y_reverse()
```

```{r}
ggplot(offline_copy, aes(ts_h, signal, fill = signal)) +
   geom_bar(stat = "identity") +
   scale_y_reverse()
```

### 5.)

Write the readData function described in Section 1.3.4. The arguments to this function are the file name and the MAC address to retain, subKMacs. Determine whether these parameters should have default values or not. The return value is data frame described in section 1.3. Use the findGlobals() function available in _codetools_ to check that the function is not relying on any global variables.

```{r}
readData <- function(file, submacs = macs) {
   
   lines <- read_lines(file)
   
   valid_lines <- lines[ validLines(lines) ]
   
   processed_lines <- lapply(valid_lines, processLine)
   
   data <- as.data.table(do.call("rbind", processed_lines),
                         stringsAsFactors = F)

   names(data) <- c("time", "scanMac", "posX", "posY", "posZ",
                       "orientation", "mac", "signal", 
                       "channel", "type")
   
   numVars <- c("time", "posX", "posY", "posZ",
                "orientation", "signal")

   data[, (numVars) := lapply(.SD, as.numeric), .SDcols = numVars]

   data <- data[ data$type == 3, ]
   data[, type := NULL]
   
   data[, rawTime := time]
   data[, time := time/1000]
   class(data$time) = c("POSIXt", "POSIXct")
   
   # drop scanMac & posZ
   data[, `:=`(scanMac = NULL, posZ = NULL)]
   
   data$angle = roundOrientation(data$orientation)
   
   data$channel = NULL
   
   data$posXY <- paste(data$posX, data$posY, sep = "-")
   
   return(data[mac %in% submacs])
}
```

### 6.)

In section 1.4.1 we calculated measures of center and location for the signal strenghts at each location x angle x access point combination (see  fig 1.9 for example).

Another possible summary statistic we can calculate is the Kolmogorov-Smirnov test-statistic for normality. If the signal strenghts are roughtly normal, then we expect the p-values to have a uniform distribution. This leads to about 5% of the p-values for the 8000 tests to fall below 0.05.

```{r}
signalSummary <-
   lapply(byLocAngleAP,
          function(oneLoc) {
             ans = oneLoc[1, ]
             ans$medSignal = median(oneLoc$signal)
             ans$avgSignal = mean(oneLoc$signal)
             ans$num = length(oneLoc$signal)
             ans$sdSignal = sd(oneLoc$signal)
             ans$iqrSignal = IQR(oneLoc$signal)
             ans
          })

offlineSigSummary <- do.call("rbind", signalSummary)

breaks <- seq(-90, -30, by = 5)

bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),
       data = offlineSummary,
       subset = mac != "00:0f:a3:39:dd:cd",
       xlab = "Mean Signal", ylab = "SD Signal")

by_pos_angle_mac <- offlineSigSummary[, .(AvgSignal = mean(signal)), by = list(mac, angle, posXY)]

with(by_pos_angle_mac, ks.test(AvgSignal, "pnorm")) # Kolmogorov-Smirnov test-statistic

by_pos_angle <- offlineSigSummary[, .(AvgSignal = mean(signal)), by = list(angle, posXY)]

with(by_pos_angle, ks.test(AvgSignal, "pnorm")) # Kolmogorov-Smirnov test-statistic

by_mac <- offline[, .(AvgSignal = mean(signal)), by = list(mac)]

with(by_mac, ks.test(AvgSignal, "pnorm")) # Kolmogorov-Smirnov test-statistic

```

### 7.)

Write the surfaceSS() function that creates plots such as those in Figure 1.10. This function takes 3 arguments: data for the offline summary data frame, mafc and angle. The parameters mac and angle are used to specify which MAC address and angle are to be selected from the data for smoothing and plotting.

```{r}

reshapeSS <- function(data, 
                      varSignal = "signal",
                      keepVars = c("posXY", "posX", "posY"),
                      sampleAngle = F) {
   
   if(sampleAngle)
      data <- data[angle == sample(data$angle, size = 1), ]
   
   byLocation <- with(data,
                 by(data, list(posXY),
                    function(x) {
                       ans <- x[1, ..keepVars]
                       avgSS <- tapply(x$signal, x$mac, mean)
                       y = matrix(avgSS, nrow = 1, dimnames = list(ans$posXY, names(avgSS)))
                       cbind(ans, y)
                    }))
   
   newDataSS <- do.call("rbind", byLocation)
   
   col_names <- colnames(newDataSS)
   to_change <- !(col_names %in% keepVars)
   
   n_cols <- length(col_names)
   start <- length(keepVars)
   
   colnames(newDataSS)[to_change] <- sapply(col_names[to_change], function(col) {
      n <- nchar(col)
      substr(col, n - 2, n)
   })
   
   newDataSS[, start:ncol(newDataSS)] <- round(newDataSS[, start:ncol(newDataSS)])
   
   return(newDataSS)
}

```

### 8.)

Consider the scatter plots in Figure 1.11. There appear to be curvature in the signal strength-distance relationship. Does a log transformation improve this relationship, i.e., make it linear? 

```{r}

offlineSummary$dist <- sqrt(diffs[, 1]^2 + diffs[, 2]^2)
offlineSummary$log_dist <- log(offlineSummary$dist)

xyplot(signal ~ dist | factor(mac) + factor(angle),
       data = offlineSummary, pch = 19, cex = 0.3,
       xlab = "distance")

xyplot(signal ~ log_dist | factor(mac) + factor(angle),
       data = offlineSummary, pch = 19, cex = 0.3,
       xlab = "log distance")

xyplot(signal ~ log_dist | factor(mac) + factor(angle),
       data = offlineSummary[angle == 315], pch = 19, cex = 0.3,
       xlab = "log distance")

xyplot(signal ~ dist | factor(mac) + factor(angle),
       data = offlineSummary[angle == 315], pch = 19, cex = 0.3,
       xlab = "distance")
```

_The log transform removes a bit of the curvature, but not a great deal._


### 9.)

The floor plan for the building (see Figure 1.1) shows 65 access points. However, the data contain 7 access points with roughly the expected number of signals (166 locations x 8 orientations x 100 replications = 146,080 measurments).

With the signal strength seen in the heat maps of Figure 1.10), we matched the access points to the corresponding MAC addresses. However, two of the MAC addresses seem to be for the same access point 00:0f:a3:39:e1:c0 and to eliminate the 00:0f:a3:39:dd:cd address.

Conduct a more thorough data analysis into these two MAC addresses. Did we make the correct decision? Does swapping out the one we kept for the one we discarded improve the prediction?

```{r}

subMacs <- names(sort(table(offline$mac), decreasing = T)) # all sub macs

angles <- seq(0, by = 45, length = 8) # all angles

surfaceSS <- function(data, filter_mac, filter_angle) {
   oneAPAngle <- data[mac == filter_mac & angle == filter_angle]

   smothSS <- Tps(oneAPAngle[, c("posX", "posY")],
                  oneAPAngle$avgSignal)
   
   vizSmooth <- predictSurface(smothSS)
   
   plot.surface(vizSmooth, type = "C")
   points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)
}

```

```{r}
surfaceSS(offlineSummary, subMacs[1], angles[1])
surfaceSS(offlineSummary, subMacs[1], angles[5])
surfaceSS(offlineSummary, subMacs[1], angles[7])

```

```{r}
surfaceSS(offlineSummary, subMacs[3], angles[2])
surfaceSS(offlineSummary, subMacs[3], angles[5])
surfaceSS(offlineSummary, subMacs[3], angles[7])

```

```{r}
surfaceSS(offlineSummary, subMacs[4], angles[2])
surfaceSS(offlineSummary, subMacs[4], angles[5])
surfaceSS(offlineSummary, subMacs[4], angles[7])
```

```{r}
surfaceSS(offlineSummary, subMacs[5], angles[2])
surfaceSS(offlineSummary, subMacs[5], angles[5])
surfaceSS(offlineSummary, subMacs[5], angles[7])

```

```{r}
surfaceSS(offlineSummary, subMacs[6], angles[2])
surfaceSS(offlineSummary, subMacs[6], angles[5])
surfaceSS(offlineSummary, subMacs[6], angles[7])
```

### 10.)

Write the selectTrain() function described in Section 1.5.2. This function has 3 parameters: angleNewObs, the angle of the new observation, signals, the training data, i.e., data in the format of offlineSummary; and m, the number of angles to include from signals.

The function returns a data frame that matches trainSS, i.e., selectTrain() calls reshapeSS().

```{r}
reshapeSS <- function(data, 
                      varSignal = "signal",
                      keepVars = c("posXY", "posX", "posY"),
                      sampleAngle = F) {
   
   if(sampleAngle)
      data <- data[angle == sample(data$angle, size = 1), ]
   
   byLocation <- with(data,
                 by(data, list(posXY),
                    function(x) {
                       ans <- x[1, ..keepVars]
                       avgSS <- tapply(x$signal, x$mac, mean)
                       y = matrix(avgSS, nrow = 1, dimnames = list(ans$posXY, names(avgSS)))
                       cbind(ans, y)
                    }))
   
   newDataSS <- do.call("rbind", byLocation)
   
   col_names <- colnames(newDataSS)
   to_change <- !(col_names %in% keepVars)
   
   n_cols <- length(col_names)
   start <- length(keepVars)
   
   colnames(newDataSS)[to_change] <- sapply(col_names[to_change], function(col) {
      n <- nchar(col)
      substr(col, n - 2, n)
   })
   
   newDataSS[, start:ncol(newDataSS)] <- round(newDataSS[, start:ncol(newDataSS)])
   
   return(newDataSS)
}
```

### 11.)

We use Euclidean distance to find the distance between the signal strength vectors. However, Euclidean distance is not robust in taht it is sensitive to outliers. Consider other metrics such as the $L_1$ distance, i.e., the absolute value of the difference. Modify the findNN() functions in sections 1.5.3 to use this alternative distance.

```{r}

findNN_l1 <- function(newSignal, trainSubset) {
   diffs <- apply(trainSubset[ , 4:9], 1,
                  function(x) abs(x - newSignal))
   dists <- apply(diffs, 2, function(x) sum(x))
   closest <- order(dists)
   return(trainSubset[closest, 1:3 ])
}

predXY_l1 <- function(newSignals, newAngles, trainData,
                   numAngles = 1, k = 3) {
   
   closeXY <- list(length = nrow(newSignals))
   
   for(i in 1:nrow(newSignals)) {
      trainSS <- selectTrain(newAngles[i], trainData, m = numAngles)
      
      closeXY[[i]] = 
         findNN_l1(newSignal = as.numeric(newSignals[i, ]), trainSS)
   }
   
   estXY = lapply(closeXY,
                  function(x) sapply(x[, 2:3],
                                     function(x) mean(x[1:k])))
   
   estXY <- do.call("rbind", estXY)
   
   return(estXY)
}

estXYk1_l1 <- predXY_l1(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 1)

estXYk3_l1 <- predXY_l1(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 3)

estXYk5_l1 <- predXY_l1(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 5)

error_table <- data.table(K = c(1, 3, 5), 
                          Error = c(calcError(estXYk1, actualXY), 
                                    calcError(estXYk3, actualXY), 
                                    calcError(estXYk5, actualXY)),
                          Method = "Euclidean")

error_table_l1 <- data.table(K = c(1, 3, 5),
                          Error = c(calcError(estXYk1_l1, actualXY), 
                                    calcError(estXYk3_l1, actualXY), 
                                    calcError(estXYk5_l1, actualXY)),
                          Method = "Manhattan")

errors_combined <- rbind(error_table, error_table_l1)

ggplot(errors_combined) +
   geom_bar(aes(K, Error, fill = -Error), stat = "identity") +
   facet_wrap(~Method) +
   coord_flip()

```

Does it improve predictions?

_Euclidean seems to perform better._


### 12.) 

To predict locaiton, we use the k nearest neighbors to a set of signal strenghts. We average the known (x, y) values for these neighbors. However, a better predictor might be a weighted average, wehre the weights are inversely proportional to the "distance" (in signal strength) from the test observation. This allows us to include the k points that are close, but to differentiate between them by how close they actually are.

The weights might be:

$\frac{1/d_i}{\sum^{k}_{i=1}1/d_i}$

for the i-th closest neighboring observation where $d_i$ is the distance from our new test point to this reference point (in signal strength space).

Implement this alternative prediciton method.

```{r}

predXY_weighted <- function(newSignals, newAngles, trainData,
                   numAngles = 1, k = 3) {
   
   closeXY <- list(length = nrow(newSignals))
   
   for(i in 1:nrow(newSignals)) {
      trainSS <- selectTrain(newAngles[i], trainData, m = numAngles)
      
      closeXY[[i]] = 
         findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)
   }
   
   estXY = lapply(closeXY,
               function(x) sapply(x[, 2:3],
                                  function(x) {
                                     points <- x[1:k]
                                     
                                     if(k > 1) {
                                        weights <- points/sum(points)
                                        
                                        return(sum(points * weights))
                                     } else {
                                        return(points)
                                     }
                                  }))
   
   estXY <- do.call("rbind", estXY)
   
   return(estXY)
}

estXYk10 <- predXY(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 10)

estXYk1_weighted <- predXY_weighted(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 1)

estXYk3_weighted <- predXY_weighted(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 3)

estXYk5_weighted <- predXY_weighted(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 5)

estXYk10_weighted <- predXY_weighted(newSignals = onlineSummary[, 6:11],
                  newAngles = onlineSummary[, 4],
                  offlineSummary, numAngles = 3, k = 10)

error_table <- data.table(K = c(1, 3, 5, 10),
                          Error = c(calcError(estXYk1, actualXY), 
                                    calcError(estXYk3, actualXY), 
                                    calcError(estXYk5, actualXY),
                                    calcError(estXYk10, actualXY)),
                          Method = "Mean")

error_table_weighted <- data.table(K = c(1, 3, 5, 10),
                          Error = c(calcError(estXYk1_weighted, actualXY), 
                                    calcError(estXYk3_weighted, actualXY), 
                                    calcError(estXYk5_weighted, actualXY),
                                    calcError(estXYk10_weighted, actualXY)),
                          Method = "Weighted Avg.")

errors_combined <- rbind(error_table, error_table_weighted)

ggplot(errors_combined) +
   geom_bar(aes(K, Error, fill = -Error), stat = "identity") +
   facet_wrap(~Method) +
   coord_flip()

errors_combined
```

Does this improve the predictions?

_The weighted avg approach is of course identical in the base case (k=1), however, it performs slightly worse than the simple average in the k=3 and k=5 cases. I even upped k to 10 (the best CV k), and we still see simple average beats the weighted approach._

### 13.)

In Section 1.5.4 we used cross-validation to choose k, the number of neighbors. Another parameter to choose is the number of angles at which the signal strength was measured.

Use cross-validation to select this value.

Also, consider N + K cross-validation pairs.

```{r}

v <- 11 # n-folds
N <- 8 # num angles
K <- 20 # also choose k

nk_err <- matrix(rep(0, N * K), nrow = N, ncol = K)

for(j in 1:v) {
   onlineFold <- subset(onlineCVSummary,
                        posXY %in% permuteLocs[, j])
   offlineFold <- subset(offlineSummary,
                         posXY %in% permuteLocs[ , -j])
   
   actualFold <- onlineFold[, c("posX", "posY")]
   
   for(k in 1:K) {
      for(n in 1:N) {
         estFold <- predXY(newSignals = onlineFold[, 6:11],
                           newAngles = onlineFold[, 4],
                           offlineFold, numAngles = n, k = k)
         
         nk_err[n, k] <- calcError(estFold, actualFold)
      }
   }
}

nk_err

which(nk_err == min(nk_err), arr.ind = TRUE)

head(order(nk_err))

# 1, 6
# 1, 5
# 1, 7
# 4, 8

n_take <- 20
cv_results <- data.table(nk_err)[, 1:n_take]
colnames(cv_results) <- sapply(1:n_take, function(x) {paste0("k=", x)})
cv_results[, Angles := .I]

min_cv <- which(nk_err == min(nk_err), arr.ind = TRUE)

cv_results_flat <- melt(cv_results, id.vars = "Angles")

ggplot(cv_results_flat, aes(variable, value, group = Angles)) +
   geom_line(aes(col = Angles)) +
   labs("K, N Cross-Validation Results")
```

The cross-validation of both N and K yield the optimal N at _1_ and K at _6_. 4 of the top 5 CV results have _N=1_.

