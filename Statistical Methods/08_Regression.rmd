---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Regression}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 8}
   \fancyfoot[C]{\rmfamily\color{headergrey}Statistical Methods}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}

knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")

```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)

assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode, "rtvs"), "data.table")

library(here, quietly = T, warn.conflicts = F)
library(ggplot2, quietly = T, warn.conflicts = F)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(WRS, quietly = TRUE, warn.conflicts = FALSE)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/datasets/")

```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

#### Chapter 8

### 8.1

For the following data, use R to verify that the least squares regresion line is $\hat{Y} = 1.8X - 8.5$

X: 5, 8, 9, 7, 14
Y: 3, 1, 6, 7, 19

```{r, echo = T}
dat <- data.table(
   X = c(5, 8, 9, 7, 14),
   Y = c(3, 1, 6, 7, 19))

lm(Y ~ X, data = dat)
```

Also verify that the Theil-Sen estimator, the slope is estimated to be 1.746 and the intercept is estimated to be -7.968.

```{r, echo = T}
tsreg(dat$X, dat$Y)$coef
```

### 8.2

Using the R function _lsfit_, compute the residuals using the data in E1,

Verify that if you square and sum the residusls, you get 46.585.

```{r, echo = T}
res <- lsfit(dat$X, dat$Y)$resid

sum(res^2)
```

### 8.3

Verify that for the data in E1, if you use $\hat{Y} = 2X - 9$, the sum of the squared residuals is greater than 46.584.

```{r, echo = T}
Yhat <- 2*dat$X - 9
res <- sum( (dat$Y - Yhat)^2 )
stopifnot(res > 46.583)
res

```

Why would you expect a value greater than 46.584?

_The x coefficent increased._

### 8.4

Suppose that based on $n = 25$ values, $s^2_x = 12$ and $\sum(X_i - \bar{X})(Y_i - \bar{Y}) = 144$.

What is the slope of the least squares regression?

$A = 144, C = (n - 1)s^2_x = 288, b_1 = A / C = 144/288 = .5$

### 8.5

The following table reports breast cancer rates plus levels of solar radiation (in calories per day) for various cities in the United States. The data are stored in the file cancer_rate_dat.txt.

```{r, echo =T}

dat <- data.table::fread(paste0(data.dir, "cancer_rate_dat.txt"), fill = T, sep = "&")

dat
```

Fit a OLS regression to predict cancer rates and comment on what this suggests.

```{r, echo = T}
fit <- lm(Rate ~ calories, data = dat)

plot(fit)
```

### 8.6

For the following data, use R to compute the least squares regresion line for predicting GPA given SAT.

SAT: 500, 530, 590, 660, 610, 700, 570, 640
GPA: 2.3, 3.1, 2.6, 3.0, 2.4, 3.3, 2.6, 3.5

```{r, echo = T}
dat <- data.table(
   SAT = c(500, 530, 590, 660, 610, 700, 570, 640),
   GPA = c(2.3, 3.1, 2.6, 3.0, 2.4, 3.3, 2.6, 3.5)
)

fit <- lm(GPA ~ SAT, data = dat)
coef(fit)
```

### 8.7

Compute the residuals for the data used in the previous problem and verify that sum to zero.

```{r, echo = T}
round(sum((dat$GPA - fit$fitted.values)), 5)
```

### 8.8

For the following data, use R to compute the least squares regression line for predicting Y from X.

X: 40, 41, 42, 43, 44, 45, 46
Y: 1.62, 1.63, 1.90, 2.64, 2.05, 2.13, 1.94

```{r, echo = T}
dat <- data.table(
   X = c(40, 41, 42, 43, 44, 45, 46),
   Y = c(1.62, 1.63, 1.90, 2.64, 2.05, 2.13, 1.94)
)

summary(fit <- lm(Y ~ X, data = dat))
```

```{r, echo = T, fig.width=8, fig.height=3.5}
ggplot(dat, aes(X, Y)) +
   geom_point() +
   geom_smooth(method = "lm")
```

### 8.9

In exercise 5, what would be the least squares estimate of the cancer rate given a solar radiation of 600?

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table::fread(paste0(data.dir, "cancer_rate_dat.txt"), fill = T, sep = "&")

ggplot(dat, aes(calories, Rate)) +
   geom_point() +
   geom_smooth(method = "lm")

fit <- lm(Rate ~ calories, data = dat)

coef(fit)

39.99 -0.037*600
```
Why might this be unreasonable?

_Because 600 is outside of the bounds of seen values (extrapolation)._

### 8.10

Maximal oxygen uptake (MOU) is a measure of an individual's physical fitness. You want to know how MOU is related to how fast someone can run a mile. Suppose you randomly sample six athletes and get:

MOU (kl/kg): 63.3, 60.1, 53.6, 58.8, 67.5, 62.5
Time (seconds): 241.5, 249.8, 246.1, 232.4, 237.2, 238.4

Compute the least squres regression line and comment on what the results suggest.

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   MOU = c(63.3, 60.1, 53.6, 58.8, 67.5, 62.5),
   Time = c(241.5, 249.8, 246.1, 232.4, 237.2, 238.4)
)

fit <- lm(Time ~ MOU, data = dat)

ggplot(dat, aes(MOU, Time)) +
   geom_point() +
   geom_smooth(method = "lm") +
   labs(title = "Time vs. MOU")

```

Generally, time decreases as MOU increases.

### 8.11

Verify that for the following pairs of points, the least squares regression line has a slope of zero. Plot the points and comment on the assumption that the regression line is straight.

X: 1, 2, 3, 4, 5, 6
Y: 1, 4, 7, 7, 4, 1

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   X = c(1, 2, 3, 4, 5, 6),
   Y = c(1, 4, 7, 7, 4, 1)
)

fit <- lm(Y ~ X, data = dat)

coef(fit)
```

### 8.12

Repeat the last exercise, only for points:

X: 1, 2, 3, 4, 5, 6
Y: 4, 5, 6, 7, 8, 2

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   X = c(1, 2, 3, 4, 5, 6),
   Y = c(4, 5, 6, 7, 8, 2)
)

fit <- lm(Y ~ X, data = dat)

coef(fit)
```

### 8.13

Vitamin A is required for good health. However, one bite of polar bear liver results in death becasue it contains a high concentration of vitamin A. Comment on this fact in terms of extrapolation.

_Vitaim A as a function of health is a bounded variable. Predicting a response when the independent variables are far from what has been observed can lead to undefined results (death)._

### 8.14

Socket et al. (1987) report data related to patterns of residual insulin secretion in children. A portion of the study was concerned with whether age can be used to predict the logarithm of C-peptide concentrations at diagnosis. The observed values are (data file), Replicate the _LOESS_ smoothed curve in fig 8.4.1.

```{r, echo = T, fig.width=8, fig.height=2.5}
dat <- data.table::fread(paste0(data.dir, "diabetes_sockett_dat.txt"), fill = T)

ggplot(dat, aes(age, pep)) +
   geom_point() +
   geom_smooth(method = "loess")

```

### 8.15
   
For the data in the last exercise, use R to verify that a least squares regression line using only X values (age) less than or equal to 7 yields a p-value equal to 0.026 when using the R function _olsch4_. Also verify that the p-value, when using the Theil-Sen estimator, is 0.0233.

```{r, echo = T}
dat2 <- dat[age <= 7]

WRS::olshc4(dat2$age, dat2$pep)

WRS::regci(dat2$age, dat2$pep)
```

### 8.16

For the reading data in Table 8.5, verify that the R function spearci returns a p-value equal to 0.014 and that scorci returns a p-value equal to 0.002. Based on the plot returned by scorci, why is it not suprising that these two function give similar results?

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table::fread(paste0(data.dir, "read_dat.txt"), fill = T, skip = 13)

spearci( dat[,4], dat[,8])

scorciMC( dat[,4], dat[,8])

```

### 8.17

Given that $b_1 = -1.5, n = 10, s^2_{y,x} = 35, \sum(X_i - \bar{X})^2 = 140$, assume normality and homosecedasiticy and compute a 0.95 confidence interval for slope, $\beta_1$.

```{r, echo = T}
b1 <- -1.5; n <- 10; se <- sqrt(35 / 140)
alpha <- .05

b1 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * se
```

### 8.18

Repeat the previous problem, only compute a 0.98 confidence interval.

```{r, echo = T}
b1 <- -1.5; n <- 10; se <- sqrt(35 / 140)
alpha <- 1 - .98

b1 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * se
```

### 8.19

Based on results convered in the previous chapters, speculate about why the confidence intervals computed in the last two problems might be inaccurate.

_Least squares regression can be negatively impacted by non-normality, heteroscedasticity and outliers._

### 8.20

Assume normality and homoscedasticity and suppose $n = 30, \sum{(X_i)} = 15, \sum(Y_i) = 30, \sum(X_i - \bar{X})(Y_i - \bar{Y}) = 30, \sum(X_i - \bar{X})^2 = 10$

```{r, echo = T}
n <- 30; A <- 30; C <- 10;
xbar <- 15 / 30; ybar <- 30 / 30

slope <- A / C
intercept <- ybar - xbar*slope

```

Determine the least squares estimates of the slope and intercept.

$\beta_1 = `r slope`, \beta_0 = `r intercept`$

### 8.21

Assume normality and homoscedasticity and suppose $n = 38, \bar{Y} = 20, \sum{X^2_i} = 1,922, \bar{X} = 7, \sum{(X_i - \bar{X})(Y_i - \bar{Y})} = 180, \sum{(X_i - \bar{X})^2} = 60, s^2_{X.Y} = 121.$

a.) Determine the least squares estimate of the slope and intercept.

```{r, echo = T}
n <- 38; ybar <- 20; xbar <- 7
variance <- 121; A <- 180; C <- 60
ssx <- 1922

slope <- A / C
intercept <- ybar - xbar*slope

```

$\beta_1 = `r slope`, \beta_0 = `r intercept`$

b.) Test the hypothesis $H_0 : \beta_0 = 0, \alpha = 0.02$

```{r, echo = T}
alpha <- 0.02

slope + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * sqrt(variance / n)

Tval <- intercept * sqrt( ((n - 2) * C) / (variance  * ssx) )

crit <- qt(alpha/2, df = n - 2)

ifelse(abs(Tval) < crit, "Reject Null", "Fail to Reject")
```

c.) Compute a 0.9 confidence interval for $\beta_1$

```{r, echo = T}
alpha <- 1 - .9

slope + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * sqrt( variance / C)
```

### 8.22

Assume normality and homoscedasticity and suppose $n = 41, \bar{Y} = 10, \bar{X} = 12, \sum(X_i - \bar{X})(Y_i - \bar{Y}) = 100, \sum(X_i - \bar{X})^2 = 400, s^2_{Y.X} = 144$.

a.) Determine the least squares regression line

```{r, echo = T}
n <- 41; ybar <- 10; xbar <- 12; 
ssr <- 100; ssx <- 400; variance <- 144

b1 <- ssr / ssx

b0 <- ybar - xbar*b1
```

$\beta_0 = `r b0`, \beta_1 = `r b1`$

b.) Compute a .9 confidence interval for $\beta_1$

```{r, echo = T}
alpha <- 1 - .9

b1 + qt(c( Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * sqrt( variance / ssx)
```

### 8.23

Assume normality and homoscedasticity and suppose $n = 18, \beta_1 = 3.1, \sum{(X_i - \bar{X}})^2 = 144, s^2_{X.Y} = 36$.

Compute a 0.95 confidence interval for $\beta_1$.

```{r, echo = T}
n <- 18; b1 <- 3.1; ssx <- 144; variance <- 36
alpha <- 1 - .95

b1 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * sqrt( variance / ssx )

```

Would you conclude $\beta_1$ > 2?

_Given the confidence interval above, it seems reasonable._

### 8.24

Assume normality and homoscedasticity and suppose $n = 20, \beta_0 = 6, \sum{X^2_i} = 169, S^2_{Y.X} = 25, \sum{(X_i - \bar{X}})^2 = 90$.

Compute a .95 confidence interval for $\beta_0$.

```{r, echo = T}
n <- 20; b0 <- 6; ssx <- 169
variance <- 25; xvar <- 90
alpha <- 1 - .95

b0 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * sqrt( (variance*ssx) / (n * xvar))
```

### 8.25

Given the following quantities, find the sample correlation coefficent, r, and test $H_0 : \rho = 0$ at the indicated value for $\alpha$.

a.) $n = 27, \sum{(Y_i - \bar{Y})}^2 = 100, \sum{(X_i - \bar{X})^2} = 625, \sum(X_i -\bar{X})(Y_i - \bar{Y})^2 = 200, \alpha = 0.01$

```{r, echo = T}
n <- 27; yvar <- 100; xvar <- 625
ssr <- 200; alpha <- 0.01

r <- ssr / sqrt( yvar * xvar )

test.stat.T <- r * sqrt( (n-2)/ (1 - r^2))

crit <- qt(1 - alpha/2, df = n - 2)

ifelse(abs(test.stat.T) >= crit, "Reject Null", "Fail to Reject")
```

b.) $n = 5, \sum{(Y_i - \bar{Y})}^2 = 16, \sum{(X_i - \bar{X})^2} = 25, \sum(X_i -\bar{X})(Y_i - \bar{Y})^2 = 10, \alpha = 0.05$

```{r, echo = T}
n <- 5; yvar <- 16; xvar <- 25
ssr <- 10; alpha <- 0.05

r <- ssr / sqrt( yvar * xvar )

test.stat.T <- r * sqrt( (n-2)/ (1 - r^2))

crit <- qt(1 - alpha/2, df = n - 2)

ifelse(abs(test.stat.T) >= crit, "Reject Null", "Fail to Reject")
```

### 8.26

The high school grade-point average (X) and college grade-point (Y) for 29 randomly sampled college freshmen yielded the following results:

$\sum{(Y_i - \bar{Y})}^2 = 64, \sum{(X_i - \bar{X})}^2 = 100, \sum{(X_i - \bar{X})(Y_i - \bar{Y})} = 40$

Test $H_0 : \rho = 0, \alpha = 0.1$ and interprest the results.

```{r, echo = T}
n <- 29; yvar <- 64; xvar <- 100; ssr <- 40
alpha <- 0.1

r <- ssr / sqrt( yvar * xvar )

crit <- qt(1 - alpha/2, df = n - 2)

test.stat.T <- r * sqrt( (n - 2) / (1 - r^2) )

ifelse( abs(test.stat.T) >= crit, "Reject null", "Fail to reject")
```

### 8.27

For the previous exercise, answer the following questions:

a.) Is it reasonable to conclude that the least squares regression line has a positive slope?

_Yes, it appears there is a positive correlation._

b.) Is it possible that despite the value for r, as high school grade-point average increases, college grade-point average decreases? Explain.

_It is possible, although unlikely._

c.) What might you do, beyond considering r, to decide it is reasonable to conclude that as high school grade-point averages increase, college grade point averages increase as well?

_Plot the data to assess the visual indication of the relationship._

### 8.28

Using R, determine what happens to Pearson's correlation between X and Y if the Y values are multiplied by 3. Argue that if Y is multiplied by any constant c != 0, Pearson's correlation does not change.

```{r, echo = T}
set.seed(46)
x <- rnorm(30); y <- rnorm(30)
cor(x, y)

cor(3*x, y)

```

### 8.29

Repeat the previous problem, only determine what happens to the slope of the least squares regresion line.

```{r, echo = T}
set.seed(46)
x <- rnorm(30); y <- rnorm(30)

lsfit(x, y)$coef

lsfit(x, 3*y)$coef
```

The absolute value of the slope gets larger.

### 8.30

Consider a least squares regression line $Y = 0.52X + 2$, assume homoscedasiticy as consider the situation where the common variance is $\sigma^2 = 1$? What happens to the correlation coefficent between X and Y if instead $\sigma^2 = 2$?

_It will increase the variance so the correlation will decrease._

### 8.31

The numerator of the coefficient of determination is $\sum{(Y_i - \bar{Y}) - \sum{(Y_i - \hat{Y})}}$. Based on the least squares principal, why is the value always greater than zero?

_If $\bar{Y}$ is used to predict Y, $\sum{(Y_i - \bar{Y})}$ will be larger than $\sum{(Y_i - \hat{Y})}$._

### 8.32

Imagine a study where the correlation between some amount of an experimental drug and liver damage yields a value for r close to zero and the hypothesis $H_0 : \rho = 0$ is not rejected. Why might it be unreasonable to conclude that the two variabels are independent?

_The magnitude of the residuals, curvature, outliers._

### 8.33

Suppose $r^2 = 0.95$. Explain why this does not provide convincing evidence that the least squares line provides a good fit to a scatterplot of the points.

_Outliers can cause large $r^2$, however, be a poor fit._

### 8.34

Imagine a situation where points are removed for which the X values are judged to be outliers. Note that this restricts the range of X values. Without looking at the data, can someone predict whether Pearson's correlation will increase or decrease after these points are removed?

_No. You need to look at the data. Restricting the range of X can increase as well as decrease r._

### 8.35

If the normality assumption is violated, what effect might this have when computing confidence intervals for the slope and intercept as described in 8.4.1?

_The condence interval can be relatively long and is potentially inaccurate._

### 8.36

If the homoscedasticity assumption is violated, what effect might this have when computing confidence intervals as described in 8.4.1?

_The condence interval can be relatively inaccurate due to using the wrong standard error._