---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Regression}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 8}
   \fancyfoot[C]{\rmfamily\color{headergrey}Statistical Methods}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}

# DO NOT ADD OR REVISE CODE HERE
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, dev = 'png')
options(knitr.table.format = "latex")

```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

library(data.table, quietly = TRUE, warn.conflicts = FALSE)

assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode, "rtvs"), "data.table")

library(here, quietly = T, warn.conflicts = F)
library(ggplot2, quietly = T, warn.conflicts = F)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(WRS, quietly = TRUE, warn.conflicts = FALSE)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/datasets/")

```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

#### Chapter 8

### 8.1

For the following data, use R to verify that the least squares regresion line is $\hat{Y} = 1.8X - 8.5$

X: 5, 8, 9, 7, 14
Y: 3, 1, 6, 7, 19

```{r, echo = T}
dat <- data.table(
   X = c(5, 8, 9, 7, 14),
   Y = c(3, 1, 6, 7, 19))

lm(Y ~ X, data = dat)
```

Also verify that the Theil-Sen estimator, the slope is estimated to be 1.746 and the intercept is estimated to be -7.968.

```{r, echo = T}
tsreg(dat$X, dat$Y)$coef
```

### 8.2

Using the R function _lsfit_, compute the residuals using the data in E1,

Verify that if you square and sum the residusls, you get 46.585.

```{r, echo = T}
res <- lsfit(dat$X, dat$Y)$resid

sum(res^2)
```

### 8.3

Verify that for the data in E1, if you use $\hat{Y} = 2X - 9$, the sum of the squared residuals is greater than 46.584.

```{r, echo = T}
Yhat <- 2*dat$X - 9
res <- sum( (dat$Y - Yhat)^2 )
stopifnot(res > 46.583)
res

```

Why would you expect a value greater than 46.584?

_The x coefficent increased._

### 8.4

Suppose that based on $n = 25$ values, $s^2_x = 12$ and $\sum(X_i - \bar{X})(Y_i - \bar{Y}) = 144$.

What is the slope of the least squares regression?

$A = 144, C = (n - 1)s^2_x = 288, b_1 = A / C = 144/288 = .5$

### 8.5

The following table reports breast cancer rates plus levels of solar radiation (in calories per day) for various cities in the United States. The data are stored in the file cancer_rate_dat.txt.

```{r, echo =T}

dat <- data.table::fread(paste0(data.dir, "cancer_rate_dat.txt"), fill = T, sep = "&")

dat
```

Fit a OLS regression to predict cancer rates and comment on what this suggests.

```{r, echo = T}
fit <- lm(Rate ~ calories, data = dat)

plot(fit)
```

### 8.6

For the following data, use R to compute the least squares regresion line for predicting GPA given SAT.

SAT: 500, 530, 590, 660, 610, 700, 570, 640
GPA: 2.3, 3.1, 2.6, 3.0, 2.4, 3.3, 2.6, 3.5

```{r, echo = T}
dat <- data.table(
   SAT = c(500, 530, 590, 660, 610, 700, 570, 640),
   GPA = c(2.3, 3.1, 2.6, 3.0, 2.4, 3.3, 2.6, 3.5)
)

fit <- lm(GPA ~ SAT, data = dat)
coef(fit)
```

### 8.7

Compute the residuals for the data used in the previous problem and verify that sum to zero.

```{r, echo = T}
round(sum((dat$GPA - fit$fitted.values)), 5)
```

### 8.8

For the following data, use R to compute the least squares regression line for predicting Y from X.

X: 40, 41, 42, 43, 44, 45, 46
Y: 1.62, 1.63, 1.90, 2.64, 2.05, 2.13, 1.94

```{r, echo = T}
dat <- data.table(
   X = c(40, 41, 42, 43, 44, 45, 46),
   Y = c(1.62, 1.63, 1.90, 2.64, 2.05, 2.13, 1.94)
)

summary(fit <- lm(Y ~ X, data = dat))
```

```{r, echo = T, fig.width=8, fig.height=3.5}
ggplot(dat, aes(X, Y)) +
   geom_point() +
   geom_smooth(method = "lm")
```

### 8.9

In exercise 5, what would be the least squares estimate of the cancer rate given a solar radiation of 600?

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table::fread(paste0(data.dir, "cancer_rate_dat.txt"), fill = T, sep = "&")

ggplot(dat, aes(calories, Rate)) +
   geom_point() +
   geom_smooth(method = "lm")

fit <- lm(Rate ~ calories, data = dat)

coef(fit)

39.99 -0.037*600
```
Why might this be unreasonable?

_Because 600 is outside of the bounds of seen values (extrapolation)._

### 8.10

Maximal oxygen uptake (MOU) is a measure of an individual's physical fitness. You want to know how MOU is related to how fast someone can run a mile. Suppose you randomly sample six athletes and get:

MOU (kl/kg): 63.3, 60.1, 53.6, 58.8, 67.5, 62.5
Time (seconds): 241.5, 249.8, 246.1, 232.4, 237.2, 238.4

Compute the least squres regression line and comment on what the results suggest.

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   MOU = c(63.3, 60.1, 53.6, 58.8, 67.5, 62.5),
   Time = c(241.5, 249.8, 246.1, 232.4, 237.2, 238.4)
)

fit <- lm(Time ~ MOU, data = dat)

ggplot(dat, aes(MOU, Time)) +
   geom_point() +
   geom_smooth(method = "lm") +
   labs(title = "Time vs. MOU")

```

Generally, time decreases as MOU increases.

### 8.11

Verify that for the following pairs of points, the least squares regression line has a slope of zero. Plot the points and comment on the assumption that the regression line is straight.

X: 1, 2, 3, 4, 5, 6
Y: 1, 4, 7, 7, 4, 1

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   X = c(1, 2, 3, 4, 5, 6),
   Y = c(1, 4, 7, 7, 4, 1)
)

fit <- lm(Y ~ X, data = dat)

coef(fit)
```

### 8.12

Repeat the last exercise, only for points:

X: 1, 2, 3, 4, 5, 6
Y: 4, 5, 6, 7, 8, 2

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table(
   X = c(1, 2, 3, 4, 5, 6),
   Y = c(4, 5, 6, 7, 8, 2)
)

fit <- lm(Y ~ X, data = dat)

coef(fit)
```

### 8.13

Vitamin A is required for good health. However, one bite of polar bear liver results in death becasue it contains a high concentration of vitamin A. Comment on this fact in terms of extrapolation.

_Vitaim A as a function of health is a bounded variable. Predicting a response when the independent variables are far from what has been observed can lead to undefined results (death)._

### 8.14

Socket et al. (1987) report data related to patterns of residual insulin secretion in children. A portion of the study was concerned with whether age can be used to predict the logarithm of C-peptide concentrations at diagnosis. The observed values are (data file), Replicate the _LOESS_ smoothed curve in fig 8.4.1.

```{r, echo = T, fig.width=8, fig.height=2.5}
dat <- data.table::fread(paste0(data.dir, "diabetes_sockett_dat.txt"), fill = T)

ggplot(dat, aes(age, pep)) +
   geom_point() +
   geom_smooth(method = "loess")

```

### 8.15
   
For the data in the last exercise, use R to verify that a least squares regression line using only X values (age) less than or equal to 7 yields a p-value equal to 0.026 when using the R function _olsch4_. Also verify that the p-value, when using the Theil-Sen estimator, is 0.0233.

```{r, echo = T}
dat2 <- dat[age <= 7]

WRS::olshc4(dat2$age, dat2$pep)

WRS::regci(dat2$age, dat2$pep)
```

### 8.16

For the reading data in Table 8.5, verify that the R function spearci returns a p-value equal to 0.014 and that scorci returns a p-value equal to 0.002. Based on the plot returned by scorci, why is it not suprising that these two function give similar results?

```{r, echo = T, fig.width=8, fig.height=3.5}
dat <- data.table::fread(paste0(data.dir, "read_dat.txt"), fill = T, skip = 13)

spearci( dat[,4], dat[,8])

scorciMC( dat[,4], dat[,8])

```

### 8.17

Given that $b_1 = -1.5, n = 10, s^2_{y,x} = 35, \sum(X_i - \bar{X})^2 = 140$, assume normality and homosecedasiticy and compute a 0.95 confidence interval for slope, $\beta_1$.

```{r, echo = T}
b1 <- -1.5; n <- 10; se <- sqrt(35 / 140)
alpha <- .05

b1 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * se
```

### 8.18

Repeat the previous problem, only compute a 0.98 confidence interval.

```{r, echo = T}
b1 <- -1.5; n <- 10; se <- sqrt(35 / 140)
alpha <- 1 - .98

b1 + qt(c(Lower = alpha/2, Upper = 1 - alpha/2), df = n - 2) * se
```

### 8.19

Based on results convered in the previous chapters, speculate about why the confidence intervals computed in the last two problems might be inaccurate.

_Least squares regression can be negatively impacted by non-normality, heteroscedasticity and outliers._

### 8.20

Assume normality and homoscedasticity and suppose $n = 30, \sum{(X_i)} = 15, \sum(Y_i) = 30, \sum(X_i - \bar{X})(Y_i - \bar{Y}) = 30, \sum(X_i - \bar{X})^2 = 10$

```{r, echo = T}
n <- 30; A <- 30; C <- 10;
xbar <- 15 / 30; ybar <- 30 / 30

slope <- A / C
intercept <- ybar - xbar*slope

```

Determine the least squares estimates of the slope and intercept.

$\beta_1 = `r slope`, \beta_0 = `r intercept`$

### 8.21

Assume normality and homoscedasticity and suppose $n = 38, \bar{Y} = 20, \sum{X^2_i} = 1,922, \bar{X} = 7, \sum{(X_i - \bar{X})(Y_i - \bar{Y})} = 180, \sum{X_i - \bar{X}^2} = 60, s^2_{X, Y} = 121.$

a.) Determine the least squares estimate of the slope and intercept.

```{r, echo = T}
```

b.) Test the hypothesis $H_0 : \beta_0 = 0, \alpha = 0.05$

```{r, echo = T}
```

c.) Compute a 0.9 confidence interval for $\beta_1$

```{r, echo = T}
```

### 8.22

Assume normality and homoscedasticity and suppose $n = 41, \bar{Y} = 10, \bar{X} = 12, \sum(X_i - \bar{X})(Y_i - \bar{Y}) = 100, \sum(X_i - \bar{X})^2 = 400, s^2_{Y.X} = 144$.

a.) Determine the least squares regression line

b.) Compute a .9 confidence interval for $\beta_1$

### 8.23

Assume normality and homoscedasticity and suppose $n = 18, \beta_1 = 3.1, \sum{(X_i - \bar{X}})^2 = 144, s^2_{X.Y} = 36$.

Compute a 0.95 confidence interval for $\beta_1$.

Would you conclude $\beta_1$ > 2?

### 8.24

Assume normality and homoscedasticity and suppose $n = 20, \beta_0 = 6, \sum{X^2_i} = 169, S^2_{Y.X} = 25, \sum{(X_i - \bar{X}})^2 = 90$.

Compute a .95 confidence interval for $\beta_0$.

### 8.25

Given the following quantities, find the sample correlation coefficent, r, and test $H_0 : \rho = 0$ at the indicated value for $\alpha$.

a.) $n = 27, \sum{(Y_i - \bar{Y})}^2 = 100, \sum{(X_i - \bar{X})^2} = 625, \sum(X_i -\bar{X})(Y_i - \bar{Y})^2 = 200, \alpha = 0.01$

b.) $n = 5, \sum{(Y_i - \bar{Y})}^2 = 16, \sum{(X_i - \bar{X})^2} = 25, \sum(X_i -\bar{X})(Y_i - \bar{Y})^2 = 10, \alpha = 0.05$

### 8.26

### 8.27

### 8.28

### 8.29

### 8.30

### 8.31
### 8.32
### 8.33
### 8.34
### 8.35
### 8.36
